{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langsmith import traceable\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = TextLoader(\"../files/chapter_three.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 정의\n",
    "template = \"\"\"다음 정보를 사용하여 질문에 답변하세요:\n",
    "\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "이전 대화:\n",
    "{chat_history}\n",
    "\n",
    "답변:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 초기화\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 수동 구현\n",
    "def retrieve_docs(query):\n",
    "    return retriever.get_relevant_documents(query)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def generate_answer(input_dict):\n",
    "    context = input_dict[\"context\"]\n",
    "    question = input_dict[\"question\"]\n",
    "    chat_history = input_dict[\"chat_history\"]\n",
    "    \n",
    "    prompt = PROMPT.format(context=context, question=question, chat_history=chat_history)\n",
    "    response = llm.invoke(prompt)\n",
    "    # 응답에서 문자열 내용만 추출\n",
    "    return response.content if hasattr(response, 'content') else str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(retrieve_docs) | RunnableLambda(format_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"chat_history\": lambda x: memory.load_memory_variables({})[\"chat_history\"]\n",
    "    }\n",
    "    | RunnableLambda(generate_answer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 및 답변 함수\n",
    "def ask_question(question):\n",
    "    result = chain.invoke(question)\n",
    "    # 결과가 문자열이 아닌 경우 문자열로 변환\n",
    "    result_str = result if isinstance(result, str) else str(result)\n",
    "    memory.save_context({\"input\": question}, {\"output\": result_str})\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Aaronson 은 유죄인가요?\n",
      "답변: 제공된 텍스트에는 Aaronson이라는 사람에 대한 언급이 없습니다. 따라서 Aaronson이 유죄인지 아닌지 판단할 수 없습니다.\n",
      "\n",
      "\n",
      "질문: 그가 테이블에 어떤 메시지를 썼나요?\n",
      "답변: 제공된 텍스트에는 윈스턴이 테이블에 메시지를 썼다는 내용이 없습니다.  텍스트는 윈스턴이 체스트넛 트리 카페에 앉아 빅 브라더의 포스터를 바라보고 있는 장면을 묘사하고 있습니다.\n",
      "\n",
      "\n",
      "질문: Julia 는 누구인가요?\n",
      "답변: 제공된 텍스트에 따르면 줄리아는 윈스턴이 사랑하는 사람입니다.  그는 그녀의 존재에 대한 압도적인 환각을 경험했고, 그녀가 여전히 살아있고 도움이 필요하다고 믿고 있습니다.  그가 그녀의 이름을 부르짖는 순간은 그가 당에 대한 복종을 어기는 행위였습니다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 질문하기\n",
    "questions = [\n",
    "    \"Aaronson 은 유죄인가요?\",\n",
    "    \"그가 테이블에 어떤 메시지를 썼나요?\",\n",
    "    \"Julia 는 누구인가요?\"\n",
    "]\n",
    "\n",
    "# chain.invoke(\"Aaronson 은 유죄인가요?\")\n",
    "# ask_question(\"Aaronson 은 유죄인가요?\")\n",
    "\n",
    "for question in questions:\n",
    "    answer = ask_question(question)\n",
    "    print(f\"질문: {question}\")\n",
    "    print(f\"답변: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
