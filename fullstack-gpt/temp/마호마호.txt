


Data connection

pip install langchain-community unstructured[all-docs]
pip install unstructured

# # Install package
pip install --upgrade --quiet  "unstructured[all-docs]"
--quiet 조용히 처리, 진행 로그가 보이지 않는다.

pip install langchain-unstructured

https://platform.openai.com/tokenizer

some stuffs mentioned in video

https://turbomaze.github.io/word2vecjson/

https://www.youtube.com/watch?v=2eWuYf-aZE4


pip install chromadb

pip install --upgrade transformers
pip install --upgrade "tokenizers>=0.21,<0.22"
pip install transformers==4.47.0 tokenizers==0.21.0

pip install tokenizers==0.20.3

pip install --upgrade "tokenizers>=0.21,<0.22"
pip install --upgrade "transformers<4.47.0"

pip install tokenizers==0.20.3
pip install --upgrade "tokenizers>=0.21,<0.22"

C:\workdir\github-space-cap\GPT-study-blog\fullstack-gpt\env\Scripts


pip install -U langchain langchain-openai

https://js.langchain.com/v0.1/docs/modules/chains/document/stuff/
Stuff documents
Refine documents chain
Map reduce documents chain
Map re-rank documents chin


https://python.langchain.com/docs/modules/chains/documents/refine

https://python.langchain.com/v0.1/docs/modules/chains/

https://python.langchain.com/v0.1/docs/use_cases/summarization/

pip install faiss-gpu
pip install faiss-cpu

system
Given the following extracted parts of a long document and a question, create a final answer. 
If you don't know the answer, just say that you don't know. Don't try to make up an answer.

from langchain.vectorstores import FAISS
vectorstore = FAISS.from_documents(docs, cached_embeddings)
는 NVIDIA gpu 에서만 사용을 하는데
amd gpu 에서도 사용하려면 어떻게 해야되.

python -c "import faiss; print(faiss.__version__)"


pip install faiss-cpu

ImportError: Could not import faiss python package. 
Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).

pip install faiss-cpu
vectorstore = FAISS.from_documents(docs, cached_embeddings)
retriver = vectorstore.as_retriever()
이렇게 코딩을 했는데 아래와 같이 에러가 발생했어. 원인이 뭐야.
Failed to use model_dump to serialize <class 'langchain_core.runnables.base.RunnableSequence'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)

https://www.telelib.com/authors/O/OrwellGeorge/prose/NineteenEightyFour/part3sec3.html
~
https://www.telelib.com/authors/O/OrwellGeorge/prose/NineteenEightyFour/part3sec6.html

http://www.george-orwell.org/1984/0.html

https://nomadcoders.co/c/gpt-challenge/lobby

https://nomadcoders.co/community/thread/10451

https://docs.streamlit.io/library/get-started/create-an-app

.env\Scripts\activate

source env/Scripts/activate
.env\Scripts\activate.bat
.env\Scripts\Activate.ps1
source .env\Scripts\activate.bat
source .env/Scripts/activate.bat

source env/Scripts/deactivate
deactivate


Why are Prompt Templates useful?

1. They help us give embeddings with variables to the LLM
2. They help us reuse prompts and format them with variables
3. They help us compress prompts to save tokens
4. They help us format the responses of the LLM


What is the difference between a PromptTemplate and a ChatPromptTemplate?

1. PromptTemplate is to format chat messages and ChatPromptTemplate is to format a string
2. ChatPromptTemplate is to format chat messages and PromptTemplate is to format a string

1. Open a terminal and navigate to your project folder.
cd myproject

2. In your terminal, type:
python -m venv .venv

3. A folder named ".venv" will appear in your project. This directory is where your virtual environment and its dependencies are installed.

4. In your terminal, activate your environment with one of the following commands, depending on your operating system.
# Windows command prompt
.venv\Scripts\activate.bat

pip install -r requirements.txt

Describe Winston's appartment
Where does winston work?
How many ministries are there?


이것은 놀라운 질문입니다. 여기에서 LCEL이 없는 코드와 LCEL이 있는 코드의 모든 예를 볼 수 있습니다.

https://python.langchain.com/v0.1/docs/expression_language/why/

예를 들어, LCEL에서는 스트리밍이 더 좋습니다: https://python.langchain.com/v0.1/docs/expression_language/why/ #stream

런타임 구성 가능성: https://python.langchain.com/v0.1/docs/expression_language/why/ #runtime-configur 가능성


openAI 대신에 ollama 로 실습하고 있습니다.


streamlit run c_7_3_001.py

langchain_community 

This app is public and searchable

https://jay-1234.streamlit.app/



mistralai/Mistral-7B-v0.1


pip install -U transformers langchain-huggingface pydantic tokenizers langchain


pip install langchain-huggingface

C:\Users\username\.cache\huggingface\hub

pip install ollama langchain langchain_community
pip install -U langchain-community
pip install -U langchain-ollama



pip install -U langchain langchain-community faiss-cpu streamlit

pip install langchain langchain_community ollama



pip install qdrant-client


pip install -U langchain-community langchain-chroma chromadb

ollama pull nomic-embed-text
ollama run nomic-embed-text



ollama run mistral




@st.cache_resource(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/private_files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)
    cache_dir = LocalFileStore(f"./.cache/private_embeddings/{file.name}")
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    loader = TextLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    # st.markdown(len(docs))

    embeddings = OllamaEmbeddings(model="mistral:latest")
    vector = embeddings.embed_query("Test query")
    st.markdown(len(vector))

    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    embedded_query = cached_embeddings.embed_query("This is a test query")
    dimension = len(embedded_query)
    st.markdown(f"임베딩 차원 수: {dimension}")

    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    st.markdown(f"Embedding dimension: {vectorstore.index.d}")
    # st.markdown(f"Embedding dimension: {vectorstore._embedding_function.client.get_collection(vectorstore._collection.name).count()}")
    # st.markdown(f"Embedding dimension: {vectorstore.embedding_function.dimension}")
    # st.markdown(f"Embedding dimension: {len(vectorstore._embedding_function.embed_query('test'))}")

    retriever = vectorstore.as_retriever()
    return retriever


langchain에서 OllamaEmbeddings, ChatOllama, FAISS 사용한 예제를 만들어줘.
model은 "mistral:latest"를 사용하고 싶어.

문서를 외부 txt 파일에서 읽어서 사용을 하고 싶어.
파일 위치는 ./files/chapter_one.txt

hummus

pip install wikipedia

pip install beautifulsoup4

pip install python-magic-bin


platform.openai.com/tokenizer

https://openai.com/api/pricing

https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/
https://platform.openai.com/docs/models/gpt-4o-mini#gpt-4o-mini

google_genai.chat_models.

genai 모델 중에서 무료인 모델이 뭐가 있어?

pip install --upgrade langchain langchain-google-genai google-generativeai


문서를 외부 txt 파일에서 읽어서 4지선다형 문제 10개를 만드는 코드를 생성하고 싶어.
파일 위치는 ./files/chapter_one.txt
ChatGoogleGenerativeAI 모델은 "gemini-1.5-flash"를 사용 해줘.
import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
또 질문과 정답을 json 으로 보여주는 것도 만들어줘

# JSON 파싱
        questions_json = json.loads(response.content)
        
        # 결과 표시
        st.json(questions_json)
        
        # 문제 및 선택지 표시
        for i, q in enumerate(questions_json["questions"], 1):
            st.subheader(f"문제 {i}")
            st.write(q["question"])
            for option in q["options"]:
                st.write(option)
            st.write(f"정답: {q['correct_answer']}")
            st.write("---")
			
			
최근에 langchain_experimental에서 ollama버전으로도 function call을 만들어줘서 공유드려요.
(https://python.langchain.com/docs/integrations/chat/ollama_functions)

https://github.com/Im-younique/fullstack-gpt/blob/120139bdeae23f69f5f30cdbb9e143cfdd77bedf/pages/QuizGPT.py

잘 만들었네.
https://github.com/daehyeong2/fullstack-gpt/blob/cb46778b63ef20d06aec13febcde696e4978e514/pages/03_QuizGPT.py

Streamlit 에서 st.ballons 을 사용을 하려고 하는데 예제 코드를 작성해줘.

AzureChatOpenAI


streamlit run Home.py


GOOGLE_API_KEY = <YOUR_API_KEY_HERE>

GOOGLE_API_KEY = "your_api_key_here"


Nico's example Video Link :
https://www.youtube.com/watch?v=CWvOKHNeLMI

Whisper Github Link:
https://github.com/openai/whisper


whisper

pip install -U openai-whisper


C:\ffmpeg\bin


pip install ffmpeg-python

podcast


ffmpeg -i files/podcast.mp4 -vn files/audio.mp3


















