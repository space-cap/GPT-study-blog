{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c145dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dff7360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ë”ì¡´ë¹„ì¦ˆì˜¨'ì´ í¬í•¨ëœ PDF íŒŒì¼ ê²€ìƒ‰ ì¤‘...\n",
      "ë°œê²¬: .\\20250630_ë”ì¡´ë¹„ì¦ˆì˜¨.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def find_pdf_files(search_name=\"ë”ì¡´ë¹„ì¦ˆì˜¨\"):\n",
    "    \"\"\"PDF íŒŒì¼ ê²€ìƒ‰\"\"\"\n",
    "    print(f\"'{search_name}'ì´ í¬í•¨ëœ PDF íŒŒì¼ ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "    # í˜„ì¬ ë””ë ‰í† ë¦¬ì™€ í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ê²€ìƒ‰\n",
    "    for root, dirs, files in os.walk(\".\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\") and search_name in file:\n",
    "                full_path = os.path.join(root, file)\n",
    "                print(f\"ë°œê²¬: {full_path}\")\n",
    "                return full_path\n",
    "\n",
    "    print(\"í•´ë‹¹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# íŒŒì¼ ê²€ìƒ‰ ë° ì²˜ë¦¬\n",
    "found_file = find_pdf_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16fc2386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\n",
      "í…ìŠ¤íŠ¸ ì§€ì‹ êµ¬ì¡° ìƒì„± ì¤‘...\n",
      "í‘œ ì¶”ì¶œ ë° Q&A ìƒì„± ì¤‘...\n",
      "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'utf-8' codec can't decode byte 0xba in position 211: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "\n",
    "class PDFToKnowledgeConverter:\n",
    "    def __init__(self):\n",
    "        self.knowledge_base = []\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "        text = \"\"\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    def extract_tables_from_pdf(self, pdf_path: str) -> List[pd.DataFrame]:\n",
    "        \"\"\"PDFì—ì„œ í‘œ ì¶”ì¶œ (tabula-py ì‚¬ìš©)\"\"\"\n",
    "        try:\n",
    "            import tabula\n",
    "\n",
    "            tables = tabula.read_pdf(pdf_path, pages=\"all\", multiple_tables=True)\n",
    "            return tables\n",
    "        except ImportError:\n",
    "            print(\"tabula-pyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install tabula-py\")\n",
    "            return []\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì •ì œ\"\"\"\n",
    "        # ë¶ˆí•„ìš”í•œ ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        text = re.sub(r\"[^\\w\\sê°€-í£.,?!]\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "    def split_into_chunks(self, text: str, chunk_size: int = 500) -> List[str]:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• \"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = \" \".join(words[i : i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def table_to_qa_pairs(self, table: pd.DataFrame) -> List[Dict]:\n",
    "        \"\"\"í‘œë¥¼ Q&A ìŒìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        qa_pairs = []\n",
    "\n",
    "        # ê° í–‰ì„ ì§ˆë¬¸-ë‹µë³€ìœ¼ë¡œ ë³€í™˜\n",
    "        for index, row in table.iterrows():\n",
    "            for col in table.columns:\n",
    "                if pd.notna(row[col]):\n",
    "                    question = f\"{col}ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.\"\n",
    "                    answer = f\"{col}ì€(ëŠ”) {row[col]}ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "                    qa_pairs.append(\n",
    "                        {\n",
    "                            \"question\": question,\n",
    "                            \"answer\": answer,\n",
    "                            \"source\": \"table\",\n",
    "                            \"metadata\": {\"table_row\": index, \"column\": col},\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        return qa_pairs\n",
    "\n",
    "    def text_to_knowledge_structure(self, text_chunks: List[str]) -> List[Dict]:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì§€ì‹ êµ¬ì¡°ë¡œ ë³€í™˜\"\"\"\n",
    "        knowledge_items = []\n",
    "\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            # ê°„ë‹¨í•œ ì œëª© ì¶”ì¶œ (ì²« ë¬¸ì¥ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©)\n",
    "            sentences = chunk.split(\".\")\n",
    "            title = (\n",
    "                sentences[0][:50] + \"...\" if len(sentences[0]) > 50 else sentences[0]\n",
    "            )\n",
    "\n",
    "            knowledge_item = {\n",
    "                \"id\": f\"doc_chunk_{i}\",\n",
    "                \"title\": title,\n",
    "                \"content\": chunk,\n",
    "                \"type\": \"document\",\n",
    "                \"metadata\": {\"chunk_index\": i, \"word_count\": len(chunk.split())},\n",
    "            }\n",
    "            knowledge_items.append(knowledge_item)\n",
    "\n",
    "        return knowledge_items\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> Dict:\n",
    "        \"\"\"PDF ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "        result = {\"document_knowledge\": [], \"table_qa_pairs\": [], \"summary\": {}}\n",
    "\n",
    "        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²˜ë¦¬\n",
    "        print(\"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\")\n",
    "        raw_text = self.extract_text_from_pdf(pdf_path)\n",
    "        cleaned_text = self.clean_text(raw_text)\n",
    "        text_chunks = self.split_into_chunks(cleaned_text)\n",
    "\n",
    "        # 2. í…ìŠ¤íŠ¸ë¥¼ ì§€ì‹ êµ¬ì¡°ë¡œ ë³€í™˜\n",
    "        print(\"í…ìŠ¤íŠ¸ ì§€ì‹ êµ¬ì¡° ìƒì„± ì¤‘...\")\n",
    "        result[\"document_knowledge\"] = self.text_to_knowledge_structure(text_chunks)\n",
    "\n",
    "        # 3. í‘œ ì¶”ì¶œ ë° Q&A ìŒ ìƒì„±\n",
    "        print(\"í‘œ ì¶”ì¶œ ë° Q&A ìƒì„± ì¤‘...\")\n",
    "        tables = self.extract_tables_from_pdf(pdf_path)\n",
    "\n",
    "        for table_idx, table in enumerate(tables):\n",
    "            if not table.empty:\n",
    "                qa_pairs = self.table_to_qa_pairs(table)\n",
    "                for qa in qa_pairs:\n",
    "                    qa[\"metadata\"][\"table_index\"] = table_idx\n",
    "                result[\"table_qa_pairs\"].extend(qa_pairs)\n",
    "\n",
    "        # 4. ìš”ì•½ ì •ë³´\n",
    "        result[\"summary\"] = {\n",
    "            \"total_text_chunks\": len(text_chunks),\n",
    "            \"total_tables\": len(tables),\n",
    "            \"total_qa_pairs\": len(result[\"table_qa_pairs\"]),\n",
    "            \"total_knowledge_items\": len(result[\"document_knowledge\"]),\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def save_knowledge_base(self, knowledge_data: Dict, output_path: str):\n",
    "        \"\"\"ì§€ì‹ ë² ì´ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(knowledge_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"ì§€ì‹ ë² ì´ìŠ¤ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    # í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "    # pip install PyPDF2 pandas tabula-py\n",
    "\n",
    "    converter = PDFToKnowledgeConverter()\n",
    "\n",
    "    # PDF íŒŒì¼ ê²½ë¡œ\n",
    "    pdf_file = \".\\\\20250630_ë”ì¡´ë¹„ì¦ˆì˜¨.pdf\"\n",
    "\n",
    "    try:\n",
    "        # PDF ì²˜ë¦¬\n",
    "        knowledge_data = converter.process_pdf(pdf_file)\n",
    "\n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        print(\"\\n=== ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ===\")\n",
    "        print(f\"í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜: {knowledge_data['summary']['total_text_chunks']}\")\n",
    "        print(f\"ì¶”ì¶œëœ í‘œ ìˆ˜: {knowledge_data['summary']['total_tables']}\")\n",
    "        print(f\"ìƒì„±ëœ Q&A ìŒ ìˆ˜: {knowledge_data['summary']['total_qa_pairs']}\")\n",
    "\n",
    "        # ì§€ì‹ ë² ì´ìŠ¤ ì €ì¥\n",
    "        converter.save_knowledge_base(knowledge_data, \"knowledge_base.json\")\n",
    "\n",
    "        # ìƒ˜í”Œ ì¶œë ¥\n",
    "        if knowledge_data[\"document_knowledge\"]:\n",
    "            print(\"\\n=== ë¬¸ì„œ ì§€ì‹ ìƒ˜í”Œ ===\")\n",
    "            print(knowledge_data[\"document_knowledge\"][0])\n",
    "\n",
    "        if knowledge_data[\"table_qa_pairs\"]:\n",
    "            print(\"\\n=== í‘œ Q&A ìƒ˜í”Œ ===\")\n",
    "            print(knowledge_data[\"table_qa_pairs\"][0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ad56f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PDF ì²˜ë¦¬ ì‹œì‘ ===\n",
      "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'PDFToKnowledgeConverter' object has no attribute 'process_pdf'\n",
      "ê°€ëŠ¥í•œ í•´ê²°ë°©ë²•:\n",
      "1. JPype1 ì„¤ì¹˜: pip install jpype1\n",
      "2. tabula-py ì¬ì„¤ì¹˜: pip install --upgrade tabula-py\n",
      "3. PDF íŒŒì¼ ì¸ì½”ë”© í™•ì¸\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "\n",
    "class PDFToKnowledgeConverter:\n",
    "    def __init__(self):\n",
    "        self.knowledge_base = []\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            # ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ íŒŒì¼ ì—´ê¸°\n",
    "            with open(pdf_path, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "        return text\n",
    "\n",
    "    def extract_tables_from_pdf(self, pdf_path: str) -> List[pd.DataFrame]:\n",
    "        \"\"\"PDFì—ì„œ í‘œ ì¶”ì¶œ (ì¸ì½”ë”© ì˜µì…˜ ì¶”ê°€)\"\"\"\n",
    "        try:\n",
    "            import tabula\n",
    "\n",
    "            # ì¸ì½”ë”© ì˜µì…˜ì„ ì—¬ëŸ¬ ê°œ ì‹œë„\n",
    "            encodings = [\"utf-8\", \"cp949\", \"euc-kr\", \"cp1252\", \"iso-8859-1\"]\n",
    "\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    print(f\"ì¸ì½”ë”© {encoding} ì‹œë„ ì¤‘...\")\n",
    "                    tables = tabula.read_pdf(\n",
    "                        pdf_path, pages=\"all\", multiple_tables=True, encoding=encoding\n",
    "                    )\n",
    "                    print(f\"ì„±ê³µ: {encoding} ì¸ì½”ë”©ìœ¼ë¡œ í‘œ ì¶”ì¶œ ì™„ë£Œ\")\n",
    "                    return tables\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "\n",
    "            print(\"ëª¨ë“  ì¸ì½”ë”© ë°©ì‹ ì‹¤íŒ¨, ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\")\n",
    "            return []\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"tabula-pyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            print(\"ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:\")\n",
    "            print(\"pip install tabula-py\")\n",
    "            print(\"pip install jpype1\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"í‘œ ì¶”ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "            return []\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì •ì œ (í•œê¸€ ì§€ì›)\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        # íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ë˜ í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ ìœ ì§€\n",
    "        text = re.sub(r\"[^\\w\\sê°€-í£.,?!():\\-]\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "    # ... ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ì€ ë™ì¼ ...\n",
    "\n",
    "\n",
    "# ì•ˆì „í•œ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def safe_process_pdf(pdf_path: str):\n",
    "    \"\"\"ì•ˆì „í•œ PDF ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        converter = PDFToKnowledgeConverter()\n",
    "\n",
    "        print(\"=== PDF ì²˜ë¦¬ ì‹œì‘ ===\")\n",
    "        knowledge_data = converter.process_pdf(pdf_path)\n",
    "\n",
    "        print(\"\\n=== ì²˜ë¦¬ ê²°ê³¼ ===\")\n",
    "        print(f\"í…ìŠ¤íŠ¸ ì²­í¬: {knowledge_data['summary']['total_text_chunks']}ê°œ\")\n",
    "        print(f\"ì¶”ì¶œëœ í‘œ: {knowledge_data['summary']['total_tables']}ê°œ\")\n",
    "        print(f\"Q&A ìŒ: {knowledge_data['summary']['total_qa_pairs']}ê°œ\")\n",
    "\n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        output_file = \"buzzon_knowledge_base.json\"\n",
    "        converter.save_knowledge_base(knowledge_data, output_file)\n",
    "\n",
    "        return knowledge_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(\"ê°€ëŠ¥í•œ í•´ê²°ë°©ë²•:\")\n",
    "        print(\"1. JPype1 ì„¤ì¹˜: pip install jpype1\")\n",
    "        print(\"2. tabula-py ì¬ì„¤ì¹˜: pip install --upgrade tabula-py\")\n",
    "        print(\"3. PDF íŒŒì¼ ì¸ì½”ë”© í™•ì¸\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file = \".\\\\20250630_ë”ì¡´ë¹„ì¦ˆì˜¨.pdf\"\n",
    "    result = safe_process_pdf(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953a7310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– PDF to ì±—ë´‡ ì§€ì‹ êµ¬ì¡° ë³€í™˜ê¸°\n",
      "==================================================\n",
      "ğŸ” íŒŒì¼ ê²½ë¡œ í™•ì¸: 20250630_ë”ì¡´ë¹„ì¦ˆì˜¨.pdf\n",
      "ğŸ“ ì ˆëŒ€ ê²½ë¡œ: c:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\lee\\20250630_ë”ì¡´ë¹„ì¦ˆì˜¨.pdf\n",
      "ğŸ“Š íŒŒì¼ í¬ê¸°: 1,741,038 bytes (1.66 MB)\n",
      "\n",
      "==================================================\n",
      "ğŸš€ PDF ì§€ì‹ êµ¬ì¡° ë³€í™˜ ì‹œì‘\n",
      "==================================================\n",
      "ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: 20250630_ë”ì¡´ë¹„ì¦ˆì˜¨.pdf\n",
      "ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\n",
      "âœ… 7ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì§€ì‹ êµ¬ì¡° ìƒì„± ì¤‘...\n",
      "ğŸ“Š í‘œ ì¶”ì¶œ ë° Q&A ìƒì„± ì¤‘...\n",
      "tabula-pyë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œ ì¶”ì¶œ ì‹œë„...\n",
      "ì¸ì½”ë”© utf-8 ì‹œë„ ì¤‘...\n",
      "utf-8 ì¸ì½”ë”© ì‹¤íŒ¨: 'utf-8' codec can't decode byte 0xb8 in position 3153: invalid start byte\n",
      "ì¸ì½”ë”© cp949 ì‹œë„ ì¤‘...\n",
      "âœ… cp949 ì¸ì½”ë”©ìœ¼ë¡œ 36ê°œ í‘œ ì¶”ì¶œ ì„±ê³µ\n",
      "ğŸ“‹ í‘œ 2 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (1, 1))\n",
      "ğŸ“‹ í‘œ 5 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (1, 1))\n",
      "ğŸ“‹ í‘œ 10 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (9, 24))\n",
      "ğŸ“‹ í‘œ 20 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (26, 3))\n",
      "ğŸ“‹ í‘œ 25 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (10, 13))\n",
      "ğŸ“‹ í‘œ 26 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (1, 2))\n",
      "ğŸ“‹ í‘œ 27 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (2, 3))\n",
      "ğŸ“‹ í‘œ 29 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (1, 2))\n",
      "ğŸ“‹ í‘œ 30 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (1, 2))\n",
      "ğŸ“‹ í‘œ 31 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (1, 3))\n",
      "ğŸ“‹ í‘œ 32 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (2, 3))\n",
      "ğŸ“‹ í‘œ 34 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (1, 1))\n",
      "ğŸ“‹ í‘œ 35 ì²˜ë¦¬ ì¤‘... (í¬ê¸°: (1, 2))\n",
      "âœ… PDF ì²˜ë¦¬ ì™„ë£Œ!\n",
      "\n",
      "==================================================\n",
      "ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\n",
      "==================================================\n",
      "ğŸ“ í…ìŠ¤íŠ¸ ì²­í¬: 7ê°œ\n",
      "ğŸ“Š ì¶”ì¶œëœ í‘œ: 36ê°œ\n",
      "â“ ìƒì„±ëœ Q&A ìŒ: 32ê°œ\n",
      "ğŸ“š ì´ ì§€ì‹ ì•„ì´í…œ: 7ê°œ\n",
      "ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: 10,129 ë¬¸ì\n",
      "ğŸ’¾ ì§€ì‹ ë² ì´ìŠ¤ê°€ 20250630_ë”ì¡´ë¹„ì¦ˆì˜¨_knowledge_base.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ“‹ ìƒ˜í”Œ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n",
      "==================================================\n",
      "\n",
      "ğŸ“ ë¬¸ì„œ ì§€ì‹ ìƒ˜í”Œ:\n",
      "ì œëª©: --- í˜ì´ì§€ 1 --- 2025\n",
      "ë‚´ìš©: --- í˜ì´ì§€ 1 --- 2025.06. 30 ë”ì¡´ë¹„ì¦ˆì˜¨ (012510) ë‚´ ì•ˆì— ë””ì§€í„¸ë±…í‚¹ì´ ìˆë‹¤(ft. ì •ë¶€ì •ì±… ìˆ˜í˜œ) Company Brief ì œì£¼ì€í–‰ì— ë””ì§€í„¸ ê´€ë ¨ ì„œë¹„ìŠ¤ ë“±ì´ ì ‘ëª©ë¨ì— ë”°ë¼ ì „êµ­êµ¬ ë””ì§€í„¸ë±…í¬ë¡œ ì „í™˜ë˜ë©´ì„œ ì„±ì¥ì„± ê°€ì†í™” ë™ì‚¬ ì œì£¼ì€í–‰ 2ëŒ€ì£¼ì£¼ë¡œì„œ ìµœëŒ€ ìˆ˜í˜œ ì§€ë‚œ 2021ë…„ ë™ì‚¬ì˜ ë°©ëŒ€í•œ ê¸°ì—… ë°ì´í„°ë¥¼ í™œìš©í•´ ì‹ í•œì€í–‰ì´ ì°¨ë³„í™”ëœ ê¸ˆìœµ ...\n",
      "í‚¤ì›Œë“œ: ['ë³„ë„ì˜', 'ë™ì‚¬ì™€', 'ë°”íƒ•ìœ¼ë¡œ', 'ë°ì´í„°ë¥¼', 'í™œìš©í•´']\n",
      "\n",
      "ğŸ“Š í‘œ Q&A ìƒ˜í”Œ:\n",
      "ì§ˆë¬¸: í‘œ 2ì˜ êµ¬ì¡°ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\n",
      "ë‹µë³€: í‘œ 2ì€ 1ê°œ ì—´, 1ê°œ í–‰ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—´ ì´ë¦„ì€ Unnamed: 0ì…ë‹ˆë‹¤....\n",
      "\n",
      "ğŸ‰ ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ìƒì„±ëœ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì±—ë´‡ í•™ìŠµì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class PDFToKnowledgeConverter:\n",
    "    def __init__(self):\n",
    "        self.knowledge_base = []\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            # ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ íŒŒì¼ ì—´ê¸°\n",
    "            with open(pdf_path, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    try:\n",
    "                        page_text = page.extract_text()\n",
    "                        if page_text:\n",
    "                            text += f\"\\n--- í˜ì´ì§€ {page_num + 1} ---\\n\"\n",
    "                            text += page_text + \"\\n\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"í˜ì´ì§€ {page_num + 1} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            print(f\"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "        return text\n",
    "\n",
    "    def extract_tables_from_pdf(self, pdf_path: str) -> List[pd.DataFrame]:\n",
    "        \"\"\"PDFì—ì„œ í‘œ ì¶”ì¶œ (ì¸ì½”ë”© ì˜µì…˜ ì¶”ê°€)\"\"\"\n",
    "        try:\n",
    "            import tabula\n",
    "\n",
    "            print(\"tabula-pyë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œ ì¶”ì¶œ ì‹œë„...\")\n",
    "\n",
    "            # ì¸ì½”ë”© ì˜µì…˜ì„ ì—¬ëŸ¬ ê°œ ì‹œë„\n",
    "            encodings = [\"utf-8\", \"cp949\", \"euc-kr\", \"cp1252\", \"iso-8859-1\"]\n",
    "\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    print(f\"ì¸ì½”ë”© {encoding} ì‹œë„ ì¤‘...\")\n",
    "                    tables = tabula.read_pdf(\n",
    "                        pdf_path,\n",
    "                        pages=\"all\",\n",
    "                        multiple_tables=True,\n",
    "                        encoding=encoding,\n",
    "                        lattice=True,  # ê²©ì ê¸°ë°˜ í…Œì´ë¸” ì¶”ì¶œ\n",
    "                        stream=True,  # ìŠ¤íŠ¸ë¦¼ ê¸°ë°˜ í…Œì´ë¸” ì¶”ì¶œë„ ì‹œë„\n",
    "                    )\n",
    "                    if tables:\n",
    "                        print(f\"âœ… {encoding} ì¸ì½”ë”©ìœ¼ë¡œ {len(tables)}ê°œ í‘œ ì¶”ì¶œ ì„±ê³µ\")\n",
    "                        return tables\n",
    "                except Exception as e:\n",
    "                    print(f\"{encoding} ì¸ì½”ë”© ì‹¤íŒ¨: {e}\")\n",
    "                    continue\n",
    "\n",
    "            print(\"âš ï¸ ëª¨ë“  ì¸ì½”ë”© ë°©ì‹ ì‹¤íŒ¨, ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\")\n",
    "            return []\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"âŒ tabula-pyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            print(\"ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:\")\n",
    "            print(\"pip install tabula-py\")\n",
    "            print(\"pip install jpype1\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"í‘œ ì¶”ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "            return []\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì •ì œ (í•œê¸€ ì§€ì›)\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        # íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ë˜ í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ ìœ ì§€\n",
    "        text = re.sub(r\"[^\\w\\sê°€-í£.,?!():\\-]\", \" \", text)\n",
    "\n",
    "        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def split_into_chunks(self, text: str, chunk_size: int = 500) -> List[str]:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„í• \n",
    "        sentences = re.split(r\"[.!?]\\s+\", text)\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_size = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            sentence_size = len(words)\n",
    "\n",
    "            if current_size + sentence_size <= chunk_size:\n",
    "                current_chunk += sentence + \". \"\n",
    "                current_size += sentence_size\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \". \"\n",
    "                current_size = sentence_size\n",
    "\n",
    "        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def table_to_qa_pairs(self, table: pd.DataFrame, table_index: int) -> List[Dict]:\n",
    "        \"\"\"í‘œë¥¼ Q&A ìŒìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        qa_pairs = []\n",
    "\n",
    "        if table.empty:\n",
    "            return qa_pairs\n",
    "\n",
    "        try:\n",
    "            # í‘œì˜ ê¸°ë³¸ ì •ë³´ Q&A\n",
    "            qa_pairs.append(\n",
    "                {\n",
    "                    \"question\": f\"í‘œ {table_index + 1}ì˜ êµ¬ì¡°ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "                    \"answer\": f\"í‘œ {table_index + 1}ì€ {len(table.columns)}ê°œ ì—´, {len(table)}ê°œ í–‰ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—´ ì´ë¦„ì€ {', '.join(str(col) for col in table.columns)}ì…ë‹ˆë‹¤.\",\n",
    "                    \"source\": \"table_structure\",\n",
    "                    \"metadata\": {\n",
    "                        \"table_index\": table_index,\n",
    "                        \"columns\": len(table.columns),\n",
    "                        \"rows\": len(table),\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # ê° í–‰ì˜ ë°ì´í„°ë¥¼ Q&Aë¡œ ë³€í™˜\n",
    "            for row_idx, row in table.iterrows():\n",
    "                if row_idx >= 10:  # ë„ˆë¬´ ë§ì€ Q&A ìƒì„± ë°©ì§€\n",
    "                    break\n",
    "\n",
    "                # ì²« ë²ˆì§¸ ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±\n",
    "                first_col = str(table.columns[0])\n",
    "                first_value = str(row.iloc[0]) if pd.notna(row.iloc[0]) else \"ì •ë³´ ì—†ìŒ\"\n",
    "\n",
    "                # í•´ë‹¹ í–‰ì˜ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•œ ë‹µë³€ ìƒì„±\n",
    "                answer_parts = []\n",
    "                for col, value in row.items():\n",
    "                    if pd.notna(value) and str(value).strip():\n",
    "                        answer_parts.append(f\"{col}: {value}\")\n",
    "\n",
    "                if answer_parts:\n",
    "                    question = f\"{first_col} '{first_value}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\"\n",
    "                    answer = (\n",
    "                        f\"{first_col} '{first_value}'ì˜ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \"\n",
    "                        + \", \".join(answer_parts)\n",
    "                        + \".\"\n",
    "                    )\n",
    "\n",
    "                    qa_pairs.append(\n",
    "                        {\n",
    "                            \"question\": question,\n",
    "                            \"answer\": answer,\n",
    "                            \"source\": \"table_row\",\n",
    "                            \"metadata\": {\n",
    "                                \"table_index\": table_index,\n",
    "                                \"row_index\": row_idx,\n",
    "                                \"primary_key\": first_value,\n",
    "                            },\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"í‘œ {table_index} Q&A ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "        return qa_pairs\n",
    "\n",
    "    def text_to_knowledge_structure(self, text_chunks: List[str]) -> List[Dict]:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì§€ì‹ êµ¬ì¡°ë¡œ ë³€í™˜\"\"\"\n",
    "        knowledge_items = []\n",
    "\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            if not chunk.strip():\n",
    "                continue\n",
    "\n",
    "            # ì²« ë¬¸ì¥ì„ ì œëª©ìœ¼ë¡œ ì¶”ì¶œ\n",
    "            sentences = chunk.split(\".\")\n",
    "            title = sentences[0].strip()[:100]  # ì œëª© ê¸¸ì´ ì œí•œ\n",
    "            if not title:\n",
    "                title = f\"ë¬¸ì„œ ì„¹ì…˜ {i + 1}\"\n",
    "\n",
    "            # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)\n",
    "            words = chunk.split()\n",
    "            keywords = []\n",
    "            for word in words:\n",
    "                if len(word) > 2 and word.isalpha():\n",
    "                    keywords.append(word)\n",
    "            keywords = list(set(keywords))[:10]  # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 10ê°œ\n",
    "\n",
    "            knowledge_item = {\n",
    "                \"id\": f\"doc_chunk_{i}\",\n",
    "                \"title\": title,\n",
    "                \"content\": chunk,\n",
    "                \"type\": \"document\",\n",
    "                \"keywords\": keywords,\n",
    "                \"metadata\": {\n",
    "                    \"chunk_index\": i,\n",
    "                    \"word_count\": len(chunk.split()),\n",
    "                    \"character_count\": len(chunk),\n",
    "                },\n",
    "            }\n",
    "            knowledge_items.append(knowledge_item)\n",
    "\n",
    "        return knowledge_items\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> Dict:\n",
    "        \"\"\"PDF ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "        result = {\n",
    "            \"document_knowledge\": [],\n",
    "            \"table_qa_pairs\": [],\n",
    "            \"summary\": {},\n",
    "            \"metadata\": {\n",
    "                \"source_file\": pdf_path,\n",
    "                \"processing_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        print(f\"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}\")\n",
    "\n",
    "        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²˜ë¦¬\n",
    "        print(\"ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\")\n",
    "        raw_text = self.extract_text_from_pdf(pdf_path)\n",
    "\n",
    "        if raw_text:\n",
    "            cleaned_text = self.clean_text(raw_text)\n",
    "            text_chunks = self.split_into_chunks(cleaned_text, chunk_size=300)\n",
    "            print(f\"âœ… {len(text_chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±\")\n",
    "\n",
    "            # 2. í…ìŠ¤íŠ¸ë¥¼ ì§€ì‹ êµ¬ì¡°ë¡œ ë³€í™˜\n",
    "            print(\"ğŸ”„ í…ìŠ¤íŠ¸ ì§€ì‹ êµ¬ì¡° ìƒì„± ì¤‘...\")\n",
    "            result[\"document_knowledge\"] = self.text_to_knowledge_structure(text_chunks)\n",
    "        else:\n",
    "            print(\"âš ï¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        # 3. í‘œ ì¶”ì¶œ ë° Q&A ìŒ ìƒì„±\n",
    "        print(\"ğŸ“Š í‘œ ì¶”ì¶œ ë° Q&A ìƒì„± ì¤‘...\")\n",
    "        tables = self.extract_tables_from_pdf(pdf_path)\n",
    "\n",
    "        for table_idx, table in enumerate(tables):\n",
    "            if not table.empty:\n",
    "                print(f\"ğŸ“‹ í‘œ {table_idx + 1} ì²˜ë¦¬ ì¤‘... (í¬ê¸°: {table.shape})\")\n",
    "                qa_pairs = self.table_to_qa_pairs(table, table_idx)\n",
    "                result[\"table_qa_pairs\"].extend(qa_pairs)\n",
    "\n",
    "        # 4. ìš”ì•½ ì •ë³´\n",
    "        result[\"summary\"] = {\n",
    "            \"total_text_chunks\": len(result[\"document_knowledge\"]),\n",
    "            \"total_tables\": len(tables),\n",
    "            \"total_qa_pairs\": len(result[\"table_qa_pairs\"]),\n",
    "            \"total_knowledge_items\": len(result[\"document_knowledge\"]),\n",
    "            \"original_text_length\": len(raw_text) if raw_text else 0,\n",
    "        }\n",
    "\n",
    "        print(\"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "        return result\n",
    "\n",
    "    def save_knowledge_base(self, knowledge_data: Dict, output_path: str):\n",
    "        \"\"\"ì§€ì‹ ë² ì´ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(knowledge_data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"ğŸ’¾ ì§€ì‹ ë² ì´ìŠ¤ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "\n",
    "def safe_process_pdf(pdf_path: str):\n",
    "    \"\"\"ì•ˆì „í•œ PDF ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
    "\n",
    "    # Path ê°ì²´ë¡œ ê²½ë¡œ ì²˜ë¦¬\n",
    "    path = Path(pdf_path)\n",
    "\n",
    "    print(f\"ğŸ” íŒŒì¼ ê²½ë¡œ í™•ì¸: {path}\")\n",
    "    print(f\"ğŸ“ ì ˆëŒ€ ê²½ë¡œ: {path.absolute()}\")\n",
    "\n",
    "    if not path.exists():\n",
    "        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}\")\n",
    "\n",
    "        # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ PDF íŒŒì¼ ê²€ìƒ‰\n",
    "        print(\"\\nğŸ” í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ PDF íŒŒì¼ ê²€ìƒ‰:\")\n",
    "        for pdf_file in Path(\".\").rglob(\"*.pdf\"):\n",
    "            print(f\"ğŸ“„ ë°œê²¬: {pdf_file}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "        file_size = path.stat().st_size\n",
    "        print(f\"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)\")\n",
    "\n",
    "        converter = PDFToKnowledgeConverter()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ğŸš€ PDF ì§€ì‹ êµ¬ì¡° ë³€í™˜ ì‹œì‘\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        knowledge_data = converter.process_pdf(str(path))\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\")\n",
    "        print(\"=\" * 50)\n",
    "        summary = knowledge_data[\"summary\"]\n",
    "        print(f\"ğŸ“ í…ìŠ¤íŠ¸ ì²­í¬: {summary['total_text_chunks']:,}ê°œ\")\n",
    "        print(f\"ğŸ“Š ì¶”ì¶œëœ í‘œ: {summary['total_tables']:,}ê°œ\")\n",
    "        print(f\"â“ ìƒì„±ëœ Q&A ìŒ: {summary['total_qa_pairs']:,}ê°œ\")\n",
    "        print(f\"ğŸ“š ì´ ì§€ì‹ ì•„ì´í…œ: {summary['total_knowledge_items']:,}ê°œ\")\n",
    "        print(f\"ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {summary['original_text_length']:,} ë¬¸ì\")\n",
    "\n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        output_file = path.stem + \"_knowledge_base.json\"\n",
    "        converter.save_knowledge_base(knowledge_data, output_file)\n",
    "\n",
    "        # ìƒ˜í”Œ ì¶œë ¥\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ğŸ“‹ ìƒ˜í”Œ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        if knowledge_data[\"document_knowledge\"]:\n",
    "            print(\"\\nğŸ“ ë¬¸ì„œ ì§€ì‹ ìƒ˜í”Œ:\")\n",
    "            sample = knowledge_data[\"document_knowledge\"][0]\n",
    "            print(f\"ì œëª©: {sample['title']}\")\n",
    "            print(f\"ë‚´ìš©: {sample['content'][:200]}...\")\n",
    "            print(f\"í‚¤ì›Œë“œ: {sample['keywords'][:5]}\")\n",
    "\n",
    "        if knowledge_data[\"table_qa_pairs\"]:\n",
    "            print(\"\\nğŸ“Š í‘œ Q&A ìƒ˜í”Œ:\")\n",
    "            sample = knowledge_data[\"table_qa_pairs\"][0]\n",
    "            print(f\"ì§ˆë¬¸: {sample['question']}\")\n",
    "            print(f\"ë‹µë³€: {sample['answer'][:200]}...\")\n",
    "\n",
    "        return knowledge_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(\"\\nğŸ”§ í•´ê²° ë°©ë²•:\")\n",
    "        print(\"1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜:\")\n",
    "        print(\"   pip install jpype1\")\n",
    "        print(\"   pip install tabula-py\")\n",
    "        print(\"   pip install PyPDF2\")\n",
    "        print(\"   pip install pandas\")\n",
    "        print(\"2. PDF íŒŒì¼ ì¸ì½”ë”© ë° ê¶Œí•œ í™•ì¸\")\n",
    "        print(\"3. Java ì„¤ì¹˜ í™•ì¸ (tabula-py í•„ìš”)\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„\n",
    "if __name__ == \"__main__\":\n",
    "    # ë”ì¡´ë¹„ì¦ˆì˜¨ PDF íŒŒì¼ ì²˜ë¦¬\n",
    "    pdf_file = \".\\\\20250630_ë”ì¡´ë¹„ì¦ˆì˜¨.pdf\"\n",
    "\n",
    "    print(\"ğŸ¤– PDF to ì±—ë´‡ ì§€ì‹ êµ¬ì¡° ë³€í™˜ê¸°\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    result = safe_process_pdf(pdf_file)\n",
    "\n",
    "    if result:\n",
    "        print(\"\\nğŸ‰ ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "        print(\"ìƒì„±ëœ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì±—ë´‡ í•™ìŠµì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"\\nğŸ˜ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e4c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
