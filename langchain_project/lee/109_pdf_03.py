import PyPDF2
import pandas as pd
import re
from typing import List, Dict
import json
import os
from pathlib import Path
import fitz  # PyMuPDF
from PIL import Image
import hashlib


class EnhancedPDFToKnowledgeConverter:
    def __init__(self, output_dir: str = "knowledge_output"):
        self.knowledge_base = []
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.images_dir = self.output_dir / "images"
        self.images_dir.mkdir(exist_ok=True)

    def extract_images_from_pdf(self, pdf_path: str) -> List[Dict]:
        """PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì €ì¥"""
        images_info = []

        try:
            import fitz  # PyMuPDF

            pdf_document = fitz.open(pdf_path)

            for page_num in range(len(pdf_document)):
                page = pdf_document.load_page(page_num)
                image_list = page.get_images(full=True)

                for img_index, img in enumerate(image_list):
                    # ì´ë¯¸ì§€ ì¶”ì¶œ
                    xref = img[0]
                    pix = fitz.Pixmap(pdf_document, xref)

                    if pix.n - pix.alpha < 4:  # GRAY or RGB
                        # ì´ë¯¸ì§€ íŒŒì¼ëª… ìƒì„±
                        img_filename = f"page_{page_num+1}_img_{img_index+1}.png"
                        img_path = self.images_dir / img_filename

                        # ì´ë¯¸ì§€ ì €ì¥
                        pix.save(str(img_path))

                        # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°
                        images_info.append(
                            {
                                "filename": img_filename,
                                "path": str(img_path),
                                "page": page_num + 1,
                                "index": img_index + 1,
                                "width": pix.width,
                                "height": pix.height,
                                "description": f"í˜ì´ì§€ {page_num+1}ì˜ ì´ë¯¸ì§€ {img_index+1}",
                                "tags": ["document_image", f"page_{page_num+1}"],
                            }
                        )

                    pix = None

            pdf_document.close()
            print(f"âœ… {len(images_info)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")

        except ImportError:
            print("âš ï¸ PyMuPDFê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install PyMuPDF")
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

        return images_info

    def save_readable_results(self, knowledge_data: Dict, output_name: str):
        """ê°€ë…ì„± ì¢‹ì€ í˜•íƒœë¡œ ê²°ê³¼ ì €ì¥"""

        # 1. HTML ë³´ê³ ì„œ ìƒì„±
        html_path = self.output_dir / f"{output_name}_report.html"
        self.create_html_report(knowledge_data, html_path)

        # 2. í…ìŠ¤íŠ¸ ìš”ì•½ íŒŒì¼ ìƒì„±
        txt_path = self.output_dir / f"{output_name}_summary.txt"
        self.create_text_summary(knowledge_data, txt_path)

        # 3. Excel íŒŒì¼ë¡œ í‘œ ë°ì´í„° ì €ì¥
        excel_path = self.output_dir / f"{output_name}_tables.xlsx"
        self.create_excel_tables(knowledge_data, excel_path)

        # 4. CSV íŒŒì¼ë¡œ Q&A ì €ì¥
        csv_path = self.output_dir / f"{output_name}_qa_pairs.csv"
        self.create_qa_csv(knowledge_data, csv_path)

        # 5. JSON íŒŒì¼ (ê¸°ì¡´)
        json_path = self.output_dir / f"{output_name}_knowledge.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(knowledge_data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“ ê²°ê³¼ë¬¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print(f"ğŸ“„ HTML ë³´ê³ ì„œ: {html_path}")
        print(f"ğŸ“ í…ìŠ¤íŠ¸ ìš”ì•½: {txt_path}")
        print(f"ğŸ“Š Excel í‘œ: {excel_path}")
        print(f"â“ Q&A CSV: {csv_path}")
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ í´ë”: {self.images_dir}")
        print(f"ğŸ’¾ JSON ë°ì´í„°: {json_path}")

    def create_html_report(self, knowledge_data: Dict, output_path: Path):
        """HTML ë³´ê³ ì„œ ìƒì„±"""
        html_content = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PDF ì§€ì‹ êµ¬ì¡° ë³€í™˜ ë³´ê³ ì„œ</title>
    <style>
        body {{ font-family: 'Malgun Gothic', sans-serif; margin: 40px; line-height: 1.6; }}
        .header {{ background: #f8f9fa; padding: 20px; border-radius: 8px; margin-bottom: 30px; }}
        .summary-box {{ background: #e3f2fd; padding: 15px; border-left: 4px solid #2196F3; margin: 20px 0; }}
        .section {{ margin: 30px 0; }}
        .knowledge-item {{ border: 1px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .qa-pair {{ background: #f5f5f5; padding: 10px; margin: 8px 0; border-radius: 5px; }}
        .question {{ font-weight: bold; color: #1976D2; }}
        .answer {{ margin-top: 5px; }}
        .image-gallery {{ display: flex; flex-wrap: wrap; gap: 10px; }}
        .image-item {{ border: 1px solid #ddd; padding: 10px; border-radius: 5px; text-align: center; }}
        .image-item img {{ max-width: 200px; max-height: 200px; }}
        .metadata {{ font-size: 0.9em; color: #666; margin-top: 10px; }}
        table {{ border-collapse: collapse; width: 100%; margin: 10px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ“„ PDF ì§€ì‹ êµ¬ì¡° ë³€í™˜ ë³´ê³ ì„œ</h1>
        <p><strong>ì›ë³¸ íŒŒì¼:</strong> {knowledge_data.get('metadata', {}).get('source_file', 'Unknown')}</p>
        <p><strong>ì²˜ë¦¬ ì‹œê°„:</strong> {knowledge_data.get('metadata', {}).get('processing_timestamp', 'Unknown')}</p>
    </div>

    <div class="summary-box">
        <h2>ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½</h2>
        <ul>
            <li>ğŸ“ í…ìŠ¤íŠ¸ ì²­í¬: <strong>{knowledge_data['summary']['total_text_chunks']:,}ê°œ</strong></li>
            <li>ğŸ“Š ì¶”ì¶œëœ í‘œ: <strong>{knowledge_data['summary']['total_tables']:,}ê°œ</strong></li>
            <li>â“ Q&A ìŒ: <strong>{knowledge_data['summary']['total_qa_pairs']:,}ê°œ</strong></li>
            <li>ğŸ–¼ï¸ ì¶”ì¶œëœ ì´ë¯¸ì§€: <strong>{len(knowledge_data.get('images', [])):,}ê°œ</strong></li>
            <li>ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: <strong>{knowledge_data['summary']['original_text_length']:,} ë¬¸ì</strong></li>
        </ul>
    </div>
"""

        # ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬ ì¶”ê°€
        if knowledge_data.get("images"):
            html_content += """
    <div class="section">
        <h2>ğŸ–¼ï¸ ì¶”ì¶œëœ ì´ë¯¸ì§€</h2>
        <div class="image-gallery">
"""
            for img in knowledge_data["images"]:
                img_path = Path(img["path"]).name
                html_content += f"""
            <div class="image-item">
                <img src="images/{img['filename']}" alt="{img['description']}">
                <div class="metadata">
                    <strong>{img['description']}</strong><br>
                    í¬ê¸°: {img['width']} x {img['height']}<br>
                    íƒœê·¸: {', '.join(img['tags'])}
                </div>
            </div>
"""
            html_content += """
        </div>
    </div>
"""

        # ë¬¸ì„œ ì§€ì‹ ì„¹ì…˜
        html_content += """
    <div class="section">
        <h2>ğŸ“š ë¬¸ì„œ ì§€ì‹ êµ¬ì¡°</h2>
"""
        for item in knowledge_data["document_knowledge"][:5]:  # ì²˜ìŒ 5ê°œë§Œ
            html_content += f"""
        <div class="knowledge-item">
            <h3>{item['title']}</h3>
            <p>{item['content'][:500]}...</p>
            <div class="metadata">
                <strong>í‚¤ì›Œë“œ:</strong> {', '.join(item.get('keywords', [])[:8])}<br>
                <strong>ë‹¨ì–´ ìˆ˜:</strong> {item['metadata']['word_count']}<br>
                <strong>ë¬¸ì ìˆ˜:</strong> {item['metadata']['character_count']}
            </div>
        </div>
"""

        # Q&A ì„¹ì…˜
        if knowledge_data["table_qa_pairs"]:
            html_content += """
    </div>
    
    <div class="section">
        <h2>â“ í‘œ ê¸°ë°˜ Q&A</h2>
"""
            for qa in knowledge_data["table_qa_pairs"][:10]:  # ì²˜ìŒ 10ê°œë§Œ
                html_content += f"""
        <div class="qa-pair">
            <div class="question">Q: {qa['question']}</div>
            <div class="answer">A: {qa['answer']}</div>
        </div>
"""

        html_content += """
    </div>
</body>
</html>
"""

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html_content)

    def create_text_summary(self, knowledge_data: Dict, output_path: Path):
        """í…ìŠ¤íŠ¸ ìš”ì•½ íŒŒì¼ ìƒì„±"""
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("=" * 60 + "\n")
            f.write("ğŸ“„ PDF ì§€ì‹ êµ¬ì¡° ë³€í™˜ ìš”ì•½ ë³´ê³ ì„œ\n")
            f.write("=" * 60 + "\n\n")

            f.write(
                f"ì›ë³¸ íŒŒì¼: {knowledge_data.get('metadata', {}).get('source_file', 'Unknown')}\n"
            )
            f.write(
                f"ì²˜ë¦¬ ì‹œê°„: {knowledge_data.get('metadata', {}).get('processing_timestamp', 'Unknown')}\n\n"
            )

            f.write("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:\n")
            f.write(
                f"- í…ìŠ¤íŠ¸ ì²­í¬: {knowledge_data['summary']['total_text_chunks']:,}ê°œ\n"
            )
            f.write(f"- ì¶”ì¶œëœ í‘œ: {knowledge_data['summary']['total_tables']:,}ê°œ\n")
            f.write(f"- Q&A ìŒ: {knowledge_data['summary']['total_qa_pairs']:,}ê°œ\n")
            f.write(f"- ì¶”ì¶œëœ ì´ë¯¸ì§€: {len(knowledge_data.get('images', [])):,}ê°œ\n")
            f.write(
                f"- ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {knowledge_data['summary']['original_text_length']:,} ë¬¸ì\n\n"
            )

            if knowledge_data.get("images"):
                f.write("ğŸ–¼ï¸ ì¶”ì¶œëœ ì´ë¯¸ì§€ ëª©ë¡:\n")
                for img in knowledge_data["images"]:
                    f.write(
                        f"- {img['filename']}: {img['description']} ({img['width']}x{img['height']})\n"
                    )
                f.write("\n")

            f.write("ğŸ“š ì£¼ìš” ë¬¸ì„œ ë‚´ìš© (ìƒìœ„ 3ê°œ):\n")
            for i, item in enumerate(knowledge_data["document_knowledge"][:3], 1):
                f.write(f"\n{i}. {item['title']}\n")
                f.write(f"   ë‚´ìš©: {item['content'][:200]}...\n")
                f.write(f"   í‚¤ì›Œë“œ: {', '.join(item.get('keywords', [])[:5])}\n")

    def create_excel_tables(self, knowledge_data: Dict, output_path: Path):
        """Excel íŒŒì¼ë¡œ í‘œ ë°ì´í„° ì €ì¥"""
        try:
            with pd.ExcelWriter(str(output_path), engine="openpyxl") as writer:
                # ìš”ì•½ ì‹œíŠ¸
                summary_data = {
                    "í•­ëª©": [
                        "í…ìŠ¤íŠ¸ ì²­í¬",
                        "ì¶”ì¶œëœ í‘œ",
                        "Q&A ìŒ",
                        "ì¶”ì¶œëœ ì´ë¯¸ì§€",
                        "ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´",
                    ],
                    "ê°œìˆ˜": [
                        knowledge_data["summary"]["total_text_chunks"],
                        knowledge_data["summary"]["total_tables"],
                        knowledge_data["summary"]["total_qa_pairs"],
                        len(knowledge_data.get("images", [])),
                        knowledge_data["summary"]["original_text_length"],
                    ],
                }
                pd.DataFrame(summary_data).to_excel(
                    writer, sheet_name="ìš”ì•½", index=False
                )

                # ë¬¸ì„œ ì§€ì‹ ì‹œíŠ¸
                if knowledge_data["document_knowledge"]:
                    doc_data = []
                    for item in knowledge_data["document_knowledge"]:
                        doc_data.append(
                            {
                                "ID": item["id"],
                                "ì œëª©": item["title"],
                                "ë‚´ìš©": item["content"][:500],
                                "í‚¤ì›Œë“œ": ", ".join(item.get("keywords", [])),
                                "ë‹¨ì–´ ìˆ˜": item["metadata"]["word_count"],
                                "ë¬¸ì ìˆ˜": item["metadata"]["character_count"],
                            }
                        )
                    pd.DataFrame(doc_data).to_excel(
                        writer, sheet_name="ë¬¸ì„œì§€ì‹", index=False
                    )

                # ì´ë¯¸ì§€ ì •ë³´ ì‹œíŠ¸
                if knowledge_data.get("images"):
                    img_data = []
                    for img in knowledge_data["images"]:
                        img_data.append(
                            {
                                "íŒŒì¼ëª…": img["filename"],
                                "ì„¤ëª…": img["description"],
                                "í˜ì´ì§€": img["page"],
                                "ë„ˆë¹„": img["width"],
                                "ë†’ì´": img["height"],
                                "íƒœê·¸": ", ".join(img["tags"]),
                            }
                        )
                    pd.DataFrame(img_data).to_excel(
                        writer, sheet_name="ì´ë¯¸ì§€ëª©ë¡", index=False
                    )

        except Exception as e:
            print(f"Excel íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")

    def create_qa_csv(self, knowledge_data: Dict, output_path: Path):
        """Q&A ìŒì„ CSVë¡œ ì €ì¥"""
        try:
            qa_data = []
            for qa in knowledge_data["table_qa_pairs"]:
                qa_data.append(
                    {
                        "ì§ˆë¬¸": qa["question"],
                        "ë‹µë³€": qa["answer"],
                        "ì¶œì²˜": qa["source"],
                        "í…Œì´ë¸”_ì¸ë±ìŠ¤": qa["metadata"].get("table_index", ""),
                        "í–‰_ì¸ë±ìŠ¤": qa["metadata"].get("row_index", ""),
                        "ì£¼ìš”í‚¤": qa["metadata"].get("primary_key", ""),
                    }
                )

            if qa_data:
                pd.DataFrame(qa_data).to_csv(
                    str(output_path), index=False, encoding="utf-8-sig"
                )
        except Exception as e:
            print(f"CSV íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")

    def process_pdf_enhanced(self, pdf_path: str) -> Dict:
        """ê°œì„ ëœ PDF ì²˜ë¦¬ (ì´ë¯¸ì§€ í¬í•¨)"""

        # ê¸°ì¡´ ì²˜ë¦¬ ë¡œì§ ì‹¤í–‰
        result = self.process_pdf(pdf_path)  # ê¸°ì¡´ ë©”ì„œë“œ í˜¸ì¶œ

        # ì´ë¯¸ì§€ ì¶”ì¶œ ì¶”ê°€
        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘...")
        images_info = self.extract_images_from_pdf(pdf_path)
        result["images"] = images_info

        return result


# ê¸°ì¡´ í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ì„œ ê¸°ëŠ¥ í™•ì¥
class PDFToKnowledgeConverter(EnhancedPDFToKnowledgeConverter):
    def __init__(self):
        self.knowledge_base = []

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)"""
        text = ""
        try:
            # ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ íŒŒì¼ ì—´ê¸°
            with open(pdf_path, "rb") as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text += f"\n--- í˜ì´ì§€ {page_num + 1} ---\n"
                            text += page_text + "\n"
                    except Exception as e:
                        print(f"í˜ì´ì§€ {page_num + 1} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        continue
        except Exception as e:
            print(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return text

    def extract_tables_from_pdf(self, pdf_path: str) -> List[pd.DataFrame]:
        """PDFì—ì„œ í‘œ ì¶”ì¶œ (ì¸ì½”ë”© ì˜µì…˜ ì¶”ê°€)"""
        try:
            import tabula

            print("tabula-pyë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œ ì¶”ì¶œ ì‹œë„...")

            # ì¸ì½”ë”© ì˜µì…˜ì„ ì—¬ëŸ¬ ê°œ ì‹œë„
            encodings = ["utf-8", "cp949", "euc-kr", "cp1252", "iso-8859-1"]

            for encoding in encodings:
                try:
                    print(f"ì¸ì½”ë”© {encoding} ì‹œë„ ì¤‘...")
                    tables = tabula.read_pdf(
                        pdf_path,
                        pages="all",
                        multiple_tables=True,
                        encoding=encoding,
                        lattice=True,  # ê²©ì ê¸°ë°˜ í…Œì´ë¸” ì¶”ì¶œ
                        stream=True,  # ìŠ¤íŠ¸ë¦¼ ê¸°ë°˜ í…Œì´ë¸” ì¶”ì¶œë„ ì‹œë„
                    )
                    if tables:
                        print(f"âœ… {encoding} ì¸ì½”ë”©ìœ¼ë¡œ {len(tables)}ê°œ í‘œ ì¶”ì¶œ ì„±ê³µ")
                        return tables
                except Exception as e:
                    print(f"{encoding} ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
                    continue

            print("âš ï¸ ëª¨ë“  ì¸ì½”ë”© ë°©ì‹ ì‹¤íŒ¨, ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
            return []

        except ImportError:
            print("âŒ tabula-pyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
            print("pip install tabula-py")
            print("pip install jpype1")
            return []
        except Exception as e:
            print(f"í‘œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []

    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ (í•œê¸€ ì§€ì›)"""
        if not text:
            return ""

        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        text = re.sub(r"\s+", " ", text)

        # íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ë˜ í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ ìœ ì§€
        text = re.sub(r"[^\w\sê°€-í£.,?!():\-]", " ", text)

        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r"\s+", " ", text)

        return text.strip()

    def split_into_chunks(self, text: str, chunk_size: int = 500) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if not text:
            return []

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„í• 
        sentences = re.split(r"[.!?]\s+", text)

        chunks = []
        current_chunk = ""
        current_size = 0

        for sentence in sentences:
            words = sentence.split()
            sentence_size = len(words)

            if current_size + sentence_size <= chunk_size:
                current_chunk += sentence + ". "
                current_size += sentence_size
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
                current_size = sentence_size

        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def table_to_qa_pairs(self, table: pd.DataFrame, table_index: int) -> List[Dict]:
        """í‘œë¥¼ Q&A ìŒìœ¼ë¡œ ë³€í™˜"""
        qa_pairs = []

        if table.empty:
            return qa_pairs

        try:
            # í‘œì˜ ê¸°ë³¸ ì •ë³´ Q&A
            qa_pairs.append(
                {
                    "question": f"í‘œ {table_index + 1}ì˜ êµ¬ì¡°ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
                    "answer": f"í‘œ {table_index + 1}ì€ {len(table.columns)}ê°œ ì—´, {len(table)}ê°œ í–‰ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—´ ì´ë¦„ì€ {', '.join(str(col) for col in table.columns)}ì…ë‹ˆë‹¤.",
                    "source": "table_structure",
                    "metadata": {
                        "table_index": table_index,
                        "columns": len(table.columns),
                        "rows": len(table),
                    },
                }
            )

            # ê° í–‰ì˜ ë°ì´í„°ë¥¼ Q&Aë¡œ ë³€í™˜
            for row_idx, row in table.iterrows():
                if row_idx >= 10:  # ë„ˆë¬´ ë§ì€ Q&A ìƒì„± ë°©ì§€
                    break

                # ì²« ë²ˆì§¸ ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
                first_col = str(table.columns[0])
                first_value = str(row.iloc[0]) if pd.notna(row.iloc[0]) else "ì •ë³´ ì—†ìŒ"

                # í•´ë‹¹ í–‰ì˜ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•œ ë‹µë³€ ìƒì„±
                answer_parts = []
                for col, value in row.items():
                    if pd.notna(value) and str(value).strip():
                        answer_parts.append(f"{col}: {value}")

                if answer_parts:
                    question = f"{first_col} '{first_value}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."
                    answer = (
                        f"{first_col} '{first_value}'ì˜ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. "
                        + ", ".join(answer_parts)
                        + "."
                    )

                    qa_pairs.append(
                        {
                            "question": question,
                            "answer": answer,
                            "source": "table_row",
                            "metadata": {
                                "table_index": table_index,
                                "row_index": row_idx,
                                "primary_key": first_value,
                            },
                        }
                    )

        except Exception as e:
            print(f"í‘œ {table_index} Q&A ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")

        return qa_pairs

    def text_to_knowledge_structure(self, text_chunks: List[str]) -> List[Dict]:
        """í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì§€ì‹ êµ¬ì¡°ë¡œ ë³€í™˜"""
        knowledge_items = []

        for i, chunk in enumerate(text_chunks):
            if not chunk.strip():
                continue

            # ì²« ë¬¸ì¥ì„ ì œëª©ìœ¼ë¡œ ì¶”ì¶œ
            sentences = chunk.split(".")
            title = sentences[0].strip()[:100]  # ì œëª© ê¸¸ì´ ì œí•œ
            if not title:
                title = f"ë¬¸ì„œ ì„¹ì…˜ {i + 1}"

            # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
            words = chunk.split()
            keywords = []
            for word in words:
                if len(word) > 2 and word.isalpha():
                    keywords.append(word)
            keywords = list(set(keywords))[:10]  # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 10ê°œ

            knowledge_item = {
                "id": f"doc_chunk_{i}",
                "title": title,
                "content": chunk,
                "type": "document",
                "keywords": keywords,
                "metadata": {
                    "chunk_index": i,
                    "word_count": len(chunk.split()),
                    "character_count": len(chunk),
                },
            }
            knowledge_items.append(knowledge_item)

        return knowledge_items

    def process_pdf(self, pdf_path: str) -> Dict:
        """PDF ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        result = {
            "document_knowledge": [],
            "table_qa_pairs": [],
            "summary": {},
            "metadata": {
                "source_file": pdf_path,
                "processing_timestamp": pd.Timestamp.now().isoformat(),
            },
        }

        print(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")

        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²˜ë¦¬
        print("ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        raw_text = self.extract_text_from_pdf(pdf_path)

        if raw_text:
            cleaned_text = self.clean_text(raw_text)
            text_chunks = self.split_into_chunks(cleaned_text, chunk_size=300)
            print(f"âœ… {len(text_chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±")

            # 2. í…ìŠ¤íŠ¸ë¥¼ ì§€ì‹ êµ¬ì¡°ë¡œ ë³€í™˜
            print("ğŸ”„ í…ìŠ¤íŠ¸ ì§€ì‹ êµ¬ì¡° ìƒì„± ì¤‘...")
            result["document_knowledge"] = self.text_to_knowledge_structure(text_chunks)
        else:
            print("âš ï¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # 3. í‘œ ì¶”ì¶œ ë° Q&A ìŒ ìƒì„±
        print("ğŸ“Š í‘œ ì¶”ì¶œ ë° Q&A ìƒì„± ì¤‘...")
        tables = self.extract_tables_from_pdf(pdf_path)

        for table_idx, table in enumerate(tables):
            if not table.empty:
                print(f"ğŸ“‹ í‘œ {table_idx + 1} ì²˜ë¦¬ ì¤‘... (í¬ê¸°: {table.shape})")
                qa_pairs = self.table_to_qa_pairs(table, table_idx)
                result["table_qa_pairs"].extend(qa_pairs)

        # 4. ìš”ì•½ ì •ë³´
        result["summary"] = {
            "total_text_chunks": len(result["document_knowledge"]),
            "total_tables": len(tables),
            "total_qa_pairs": len(result["table_qa_pairs"]),
            "total_knowledge_items": len(result["document_knowledge"]),
            "original_text_length": len(raw_text) if raw_text else 0,
        }

        print("âœ… PDF ì²˜ë¦¬ ì™„ë£Œ!")
        return result

    def save_knowledge_base(self, knowledge_data: Dict, output_path: str):
        """ì§€ì‹ ë² ì´ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(knowledge_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ì§€ì‹ ë² ì´ìŠ¤ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")


# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def process_pdf_with_enhanced_output(pdf_path: str):
    """í–¥ìƒëœ ì¶œë ¥ ê¸°ëŠ¥ì´ í¬í•¨ëœ PDF ì²˜ë¦¬"""

    path = Path(pdf_path)
    if not path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return None

    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_name = path.stem
        converter = PDFToKnowledgeConverter(output_dir=f"{output_name}_results")

        print("ğŸš€ PDF ì²˜ë¦¬ ì‹œì‘ (ì´ë¯¸ì§€ í¬í•¨)")
        knowledge_data = converter.process_pdf_enhanced(str(path))

        # ë‹¤ì–‘í•œ í˜•íƒœë¡œ ê²°ê³¼ ì €ì¥
        converter.save_readable_results(knowledge_data, output_name)

        print("\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ! ë‹¤ìŒ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ì„¸ìš”:")
        print("ğŸ“„ HTML ë³´ê³ ì„œ - ì›¹ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ë³´ì„¸ìš”")
        print("ğŸ“ í…ìŠ¤íŠ¸ ìš”ì•½ - ê°„ë‹¨í•œ ê°œìš”")
        print("ğŸ“Š Excel íŒŒì¼ - í‘œ ë°ì´í„°")
        print("â“ CSV íŒŒì¼ - Q&A ìŒë“¤")
        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ í´ë” - ì¶”ì¶œëœ ëª¨ë“  ì´ë¯¸ì§€")

        return knowledge_data

    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None


# ì‹¤í–‰
if __name__ == "__main__":
    pdf_file = ".\\20250630_ë”ì¡´ë¹„ì¦ˆì˜¨.pdf"
    process_pdf_with_enhanced_output(pdf_file)
