{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1680f910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f8c7dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-cb95d90c-1834-491b-9cef-f4adb2b3c69b', 'json_data': {'messages': [{'content': 'ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”: ì„œìš¸ì˜ ì¸êµ¬ëŠ”?', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002868ABFC5F0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x000002868A66A060> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002868ABFC500>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ ===\n",
      "\u001b[1m[values]\u001b[0m {'input': 'ì„œìš¸ì˜ ì¸êµ¬ëŠ”?', 'chat_history': [], 'agent_outcome': None, 'intermediate_steps': [], 'output': ''}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "âŒ [DEBUG] Agent ì‹¤í–‰ ì‹¤íŒ¨: RunnableAgent.plan() got multiple values for argument 'intermediate_steps'\n",
      "ğŸ”„ [DEBUG] Fallback ëª¨ë“œë¡œ ì „í™˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Jul 2025 13:45:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'ailysfrontier'), (b'openai-processing-ms', b'1625'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'1628'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199983'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'req_bc40a2cc68cffda59a2545aa2ca31690'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'959f105dff88ea15-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 04 Jul 2025 13:45:37 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'ailysfrontier', 'openai-processing-ms': '1625', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '1628', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199983', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '4ms', 'x-request-id': 'req_bc40a2cc68cffda59a2545aa2ca31690', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '959f105dff88ea15-ICN', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_bc40a2cc68cffda59a2545aa2ca31690\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-73d0e83c-cad3-4e47-b446-2c6e782da7c9', 'json_data': {'messages': [{'content': 'ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”: ëŒ€êµ¬ì˜ ì¸êµ¬ëŠ”?', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[updates]\u001b[0m {'agent': {'input': 'ì„œìš¸ì˜ ì¸êµ¬ëŠ”?', 'chat_history': [], 'agent_outcome': None, 'intermediate_steps': [], 'output': 'ğŸ¤– ë„êµ¬ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤:\\n2023ë…„ ê¸°ì¤€ìœ¼ë¡œ ì„œìš¸ì˜ ì¸êµ¬ëŠ” ì•½ 9ë°±ë§Œ ëª… ì •ë„ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸êµ¬ëŠ” ì§€ì†ì ìœ¼ë¡œ ë³€ë™í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìµœì‹  í†µê³„ ìë£Œë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì„œìš¸íŠ¹ë³„ì‹œì²­ì´ë‚˜ í†µê³„ì²­ì˜ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'}}\n",
      "\u001b[1m[values]\u001b[0m {'input': 'ì„œìš¸ì˜ ì¸êµ¬ëŠ”?', 'chat_history': [], 'agent_outcome': None, 'intermediate_steps': [], 'output': 'ğŸ¤– ë„êµ¬ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤:\\n2023ë…„ ê¸°ì¤€ìœ¼ë¡œ ì„œìš¸ì˜ ì¸êµ¬ëŠ” ì•½ 9ë°±ë§Œ ëª… ì •ë„ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸êµ¬ëŠ” ì§€ì†ì ìœ¼ë¡œ ë³€ë™í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìµœì‹  í†µê³„ ìë£Œë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì„œìš¸íŠ¹ë³„ì‹œì²­ì´ë‚˜ í†µê³„ì²­ì˜ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'}\n",
      "ğŸ¤– ë„êµ¬ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤:\n",
      "2023ë…„ ê¸°ì¤€ìœ¼ë¡œ ì„œìš¸ì˜ ì¸êµ¬ëŠ” ì•½ 9ë°±ë§Œ ëª… ì •ë„ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸êµ¬ëŠ” ì§€ì†ì ìœ¼ë¡œ ë³€ë™í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìµœì‹  í†µê³„ ìë£Œë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì„œìš¸íŠ¹ë³„ì‹œì²­ì´ë‚˜ í†µê³„ì²­ì˜ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "=== ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì •ë³´ (LLM Fallback) ===\n",
      "\u001b[1m[values]\u001b[0m {'input': 'ëŒ€êµ¬ì˜ ì¸êµ¬ëŠ”?', 'chat_history': [], 'agent_outcome': None, 'intermediate_steps': [], 'output': ''}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "âŒ [DEBUG] Agent ì‹¤í–‰ ì‹¤íŒ¨: RunnableAgent.plan() got multiple values for argument 'intermediate_steps'\n",
      "ğŸ”„ [DEBUG] Fallback ëª¨ë“œë¡œ ì „í™˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Jul 2025 13:45:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'ailysfrontier'), (b'openai-processing-ms', b'1092'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'1097'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'199983'), (b'x-ratelimit-reset-requests', b'15.201s'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'req_196f69a1216535d1d8bd054a5f1419bd'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'959f10697d31ea15-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 04 Jul 2025 13:45:39 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'ailysfrontier', 'openai-processing-ms': '1092', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '1097', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '199983', 'x-ratelimit-reset-requests': '15.201s', 'x-ratelimit-reset-tokens': '4ms', 'x-request-id': 'req_196f69a1216535d1d8bd054a5f1419bd', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '959f10697d31ea15-ICN', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_196f69a1216535d1d8bd054a5f1419bd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[updates]\u001b[0m {'agent': {'input': 'ëŒ€êµ¬ì˜ ì¸êµ¬ëŠ”?', 'chat_history': [], 'agent_outcome': None, 'intermediate_steps': [], 'output': 'ğŸ¤– ë„êµ¬ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤:\\nëŒ€êµ¬ì˜ ì¸êµ¬ëŠ” 2023ë…„ ê¸°ì¤€ìœ¼ë¡œ ì•½ 240ë§Œ ëª… ì •ë„ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸êµ¬ëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë³€ë™í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ëŒ€êµ¬ì˜ ì¸êµ¬ì— ëŒ€í•œ ì •í™•í•œ ìˆ˜ì¹˜ëŠ” ëŒ€êµ¬ê´‘ì—­ì‹œì²­ì´ë‚˜ í†µê³„ì²­ì˜ ê³µì‹ ìë£Œë¥¼ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.'}}\n",
      "\u001b[1m[values]\u001b[0m {'input': 'ëŒ€êµ¬ì˜ ì¸êµ¬ëŠ”?', 'chat_history': [], 'agent_outcome': None, 'intermediate_steps': [], 'output': 'ğŸ¤– ë„êµ¬ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤:\\nëŒ€êµ¬ì˜ ì¸êµ¬ëŠ” 2023ë…„ ê¸°ì¤€ìœ¼ë¡œ ì•½ 240ë§Œ ëª… ì •ë„ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸êµ¬ëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë³€ë™í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ëŒ€êµ¬ì˜ ì¸êµ¬ì— ëŒ€í•œ ì •í™•í•œ ìˆ˜ì¹˜ëŠ” ëŒ€êµ¬ê´‘ì—­ì‹œì²­ì´ë‚˜ í†µê³„ì²­ì˜ ê³µì‹ ìë£Œë¥¼ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.'}\n",
      "ğŸ¤– ë„êµ¬ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤:\n",
      "ëŒ€êµ¬ì˜ ì¸êµ¬ëŠ” 2023ë…„ ê¸°ì¤€ìœ¼ë¡œ ì•½ 240ë§Œ ëª… ì •ë„ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸êµ¬ëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë³€ë™í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ëŒ€êµ¬ì˜ ì¸êµ¬ì— ëŒ€í•œ ì •í™•í•œ ìˆ˜ì¹˜ëŠ” ëŒ€êµ¬ê´‘ì—­ì‹œì²­ì´ë‚˜ í†µê³„ì²­ì˜ ê³µì‹ ìë£Œë¥¼ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import Tool, create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "import operator\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜\n",
    "load_dotenv()\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# AgentState ì •ì˜ (ì˜¬ë°”ë¥¸ íƒ€ì… ì§€ì •)\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[List[BaseMessage], operator.add]\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "    intermediate_steps: Annotated[List[tuple], operator.add]\n",
    "    output: str  # ìµœì¢… ì¶œë ¥ìš© í•„ë“œ ì¶”ê°€\n",
    "\n",
    "\n",
    "# ê¸°ì¡´ ë„êµ¬ í•¨ìˆ˜ë“¤\n",
    "def add_two(x: str) -> str:\n",
    "    try:\n",
    "        return str(float(x) + 2)\n",
    "    except Exception:\n",
    "        raise ValueError(\"ìˆ«ìë§Œ ì…ë ¥í•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "def population_lookup(city: str) -> str:\n",
    "    data = {\n",
    "        \"ì„œìš¸\": \"9,733,509ëª… (2025ë…„ ê¸°ì¤€)\",\n",
    "        \"ë¶€ì‚°\": \"3,400,000ëª… (2025ë…„ ê¸°ì¤€)\",\n",
    "    }\n",
    "    # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰í•˜ë„ë¡ ìˆ˜ì •\n",
    "    city_lower = city.strip().lower()\n",
    "    for key, value in data.items():\n",
    "        if key.lower() == city_lower:\n",
    "            return value\n",
    "    raise ValueError(f\"{city}ì˜ ì¸êµ¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# ë„êµ¬ ë° LLM ì„¤ì •\n",
    "tools = [\n",
    "    Tool(name=\"AddTwo\", func=add_two, description=\"ìˆ«ì ì…ë ¥ ì‹œ 2ë¥¼ ë”í•©ë‹ˆë‹¤.\"),\n",
    "    Tool(\n",
    "        name=\"PopulationLookup\",\n",
    "        func=population_lookup,\n",
    "        description=\"ë„ì‹œ ì´ë¦„ ì…ë ¥ ì‹œ ì¸êµ¬ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", temperature=0.1, openai_api_key=OPEN_API_KEY, verbose=True\n",
    ")\n",
    "\n",
    "# ReAct í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "react_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:\n",
    "{tools}\n",
    "\n",
    "ë„êµ¬ ì´ë¦„: {tool_names}\n",
    "\n",
    "ë‹¤ìŒ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”:\n",
    "\n",
    "Question: ë‹µë³€í•´ì•¼ í•  ì…ë ¥ ì§ˆë¬¸\n",
    "Thought: ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ìƒê°í•´ë³´ì„¸ìš”\n",
    "Action: ìˆ˜í–‰í•  ì•¡ì…˜, [{tool_names}] ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤\n",
    "Action Input: ì•¡ì…˜ì— ëŒ€í•œ ì…ë ¥\n",
    "Observation: ì•¡ì…˜ì˜ ê²°ê³¼\n",
    "... (ì´ Thought/Action/Action Input/Observationì„ í•„ìš”í•œ ë§Œí¼ ë°˜ë³µ)\n",
    "Thought: ì´ì œ ìµœì¢… ë‹µë³€ì„ ì•Œì•˜ìŠµë‹ˆë‹¤\n",
    "Final Answer: ì›ë˜ ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€\n",
    "\n",
    "ì‹œì‘í•˜ì„¸ìš”!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ReAct Agentì™€ AgentExecutor ìƒì„±\n",
    "react_agent = create_react_agent(llm, tools, react_prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=react_agent,\n",
    "    tools=tools,\n",
    "    verbose=True,  # Agent ì‹¤í–‰ ê³¼ì • ì¶œë ¥\n",
    "    return_intermediate_steps=True,  # ì¤‘ê°„ ë‹¨ê³„ ë°˜í™˜\n",
    "    max_iterations=10,  # ë””ë²„ê¹…ìš© ìµœëŒ€ ë°˜ë³µ ì œí•œ\n",
    ")\n",
    "\n",
    "\n",
    "# Agent ë…¸ë“œ í•¨ìˆ˜ (ìˆ˜ì •ë¨)\n",
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    try:\n",
    "        # AgentExecutor ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰\n",
    "        result = agent_executor.invoke(\n",
    "            {\n",
    "                \"input\": state[\"input\"],\n",
    "                \"chat_history\": state.get(\"chat_history\", []),\n",
    "                \"intermediate_steps\": state.get(\"intermediate_steps\", []),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… [DEBUG] Agent ì‹¤í–‰ ì„±ê³µ\")\n",
    "        print(f\"ğŸ“Š [DEBUG] ì¤‘ê°„ ë‹¨ê³„ ìˆ˜: {len(result.get('intermediate_steps', []))}\")\n",
    "\n",
    "        return {\n",
    "            \"input\": state[\"input\"],\n",
    "            \"chat_history\": state.get(\"chat_history\", []),\n",
    "            \"agent_outcome\": None,  # AgentExecutorëŠ” ì´ë¯¸ ì™„ë£Œëœ ê²°ê³¼ë¥¼ ë°˜í™˜\n",
    "            \"intermediate_steps\": result.get(\"intermediate_steps\", []),\n",
    "            \"output\": result[\"output\"],  # ìµœì¢… ê²°ê³¼ë¥¼ output í•„ë“œì— ì €ì¥\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ [DEBUG] Agent ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}\")\n",
    "        print(f\"ğŸ”„ [DEBUG] Fallback ëª¨ë“œë¡œ ì „í™˜\")\n",
    "\n",
    "        # Fallback ì²˜ë¦¬\n",
    "        response = llm.invoke(f\"ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”: {state['input']}\")\n",
    "        fallback_result = f\"ğŸ¤– ë„êµ¬ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤:\\n{response.content}\"\n",
    "\n",
    "        return {\n",
    "            \"input\": state[\"input\"],\n",
    "            \"chat_history\": state.get(\"chat_history\", []),\n",
    "            \"agent_outcome\": None,\n",
    "            \"intermediate_steps\": state.get(\"intermediate_steps\", []),\n",
    "            \"output\": fallback_result,\n",
    "        }\n",
    "\n",
    "\n",
    "# StateGraph ìƒì„±\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "\n",
    "# ì‹œì‘ì ê³¼ ì¢…ë£Œì  ì„¤ì •\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.add_edge(\"agent\", END)\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "smart_agent = graph.compile(debug=True)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "if __name__ == \"__main__\":\n",
    "    # ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´\n",
    "    print(\"=== ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ ===\")\n",
    "    result1 = smart_agent.invoke(\n",
    "        {\n",
    "            \"input\": \"ì„œìš¸ì˜ ì¸êµ¬ëŠ”?\",\n",
    "            \"chat_history\": [],\n",
    "            \"agent_outcome\": None,\n",
    "            \"intermediate_steps\": [],\n",
    "            \"output\": \"\",\n",
    "        }\n",
    "    )\n",
    "    print(result1[\"output\"])\n",
    "\n",
    "    print(\"\\n=== ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì •ë³´ (LLM Fallback) ===\")\n",
    "    result2 = smart_agent.invoke(\n",
    "        {\n",
    "            \"input\": \"ëŒ€êµ¬ì˜ ì¸êµ¬ëŠ”?\",\n",
    "            \"chat_history\": [],\n",
    "            \"agent_outcome\": None,\n",
    "            \"intermediate_steps\": [],\n",
    "            \"output\": \"\",\n",
    "        }\n",
    "    )\n",
    "    print(result2[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab4982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
