{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1680f910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f8c7dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ ===\n",
      "ğŸ¤– ë„êµ¬ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤:\n",
      "2023ë…„ ê¸°ì¤€ìœ¼ë¡œ ì„œìš¸ì˜ ì¸êµ¬ëŠ” ì•½ 9ë°±ë§Œ ëª… ì •ë„ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸êµ¬ëŠ” ì§€ì†ì ìœ¼ë¡œ ë³€ë™í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìµœì‹  í†µê³„ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì„œìš¸íŠ¹ë³„ì‹œì²­ì´ë‚˜ í†µê³„ì²­ì˜ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "=== ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì •ë³´ (LLM Fallback) ===\n",
      "ğŸ¤– ë„êµ¬ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤:\n",
      "ëŒ€êµ¬ì˜ ì¸êµ¬ëŠ” 2023ë…„ ê¸°ì¤€ìœ¼ë¡œ ì•½ 240ë§Œ ëª… ì •ë„ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸êµ¬ëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë³€ë™í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ëŒ€êµ¬ì— ëŒ€í•œ ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import Tool, create_react_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜\n",
    "load_dotenv()\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# AgentState ì •ì˜\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[List[BaseMessage], operator.add]\n",
    "    agent_outcome: str\n",
    "    intermediate_steps: List[tuple]\n",
    "\n",
    "\n",
    "# ê¸°ì¡´ ë„êµ¬ í•¨ìˆ˜ë“¤\n",
    "def add_two(x: str) -> str:\n",
    "    try:\n",
    "        return str(float(x) + 2)\n",
    "    except Exception:\n",
    "        raise ValueError(\"ìˆ«ìë§Œ ì…ë ¥í•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "def population_lookup(city: str) -> str:\n",
    "    data = {\n",
    "        \"ì„œìš¸\": \"9,733,509ëª… (2025ë…„ ê¸°ì¤€)\",\n",
    "        \"ë¶€ì‚°\": \"3,400,000ëª… (2025ë…„ ê¸°ì¤€)\",\n",
    "    }\n",
    "    result = data.get(city.strip().lower())\n",
    "    if result:\n",
    "        return result\n",
    "    else:\n",
    "        raise ValueError(f\"{city}ì˜ ì¸êµ¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# ë„êµ¬ ë° LLM ì„¤ì •\n",
    "tools = [\n",
    "    Tool(name=\"AddTwo\", func=add_two, description=\"ìˆ«ì ì…ë ¥ ì‹œ 2ë¥¼ ë”í•©ë‹ˆë‹¤.\"),\n",
    "    Tool(\n",
    "        name=\"PopulationLookup\",\n",
    "        func=population_lookup,\n",
    "        description=\"ë„ì‹œ ì´ë¦„ ì…ë ¥ ì‹œ ì¸êµ¬ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", temperature=0, openai_api_key=OPEN_API_KEY, verbose=True\n",
    ")\n",
    "\n",
    "# ReAct í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "react_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:\n",
    "{tools}\n",
    "\n",
    "ë„êµ¬ ì´ë¦„: {tool_names}\n",
    "\n",
    "ë‹¤ìŒ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”:\n",
    "\n",
    "Question: ë‹µë³€í•´ì•¼ í•  ì…ë ¥ ì§ˆë¬¸\n",
    "Thought: ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ìƒê°í•´ë³´ì„¸ìš”\n",
    "Action: ìˆ˜í–‰í•  ì•¡ì…˜, [{tool_names}] ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤\n",
    "Action Input: ì•¡ì…˜ì— ëŒ€í•œ ì…ë ¥\n",
    "Observation: ì•¡ì…˜ì˜ ê²°ê³¼\n",
    "... (ì´ Thought/Action/Action Input/Observationì„ í•„ìš”í•œ ë§Œí¼ ë°˜ë³µ)\n",
    "Thought: ì´ì œ ìµœì¢… ë‹µë³€ì„ ì•Œì•˜ìŠµë‹ˆë‹¤\n",
    "Final Answer: ì›ë˜ ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€\n",
    "\n",
    "ì‹œì‘í•˜ì„¸ìš”!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ReAct Agent ìƒì„±\n",
    "react_agent = create_react_agent(llm, tools, react_prompt)\n",
    "\n",
    "\n",
    "# Agent ë…¸ë“œ í•¨ìˆ˜\n",
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    try:\n",
    "        # Agent ì‹¤í–‰\n",
    "        result = react_agent.invoke(\n",
    "            {\n",
    "                \"input\": state[\"input\"],\n",
    "                \"tools\": tools,\n",
    "                \"tool_names\": [tool.name for tool in tools],\n",
    "                \"agent_scratchpad\": \"\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input\": state[\"input\"],\n",
    "            \"chat_history\": state.get(\"chat_history\", []),\n",
    "            \"agent_outcome\": result,\n",
    "            \"intermediate_steps\": state.get(\"intermediate_steps\", []),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Fallback ì²˜ë¦¬\n",
    "        response = llm.invoke(f\"ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”: {state['input']}\")\n",
    "        fallback_result = f\"ğŸ¤– ë„êµ¬ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤:\\n{response.content}\"\n",
    "\n",
    "        return {\n",
    "            \"input\": state[\"input\"],\n",
    "            \"chat_history\": state.get(\"chat_history\", []),\n",
    "            \"agent_outcome\": fallback_result,\n",
    "            \"intermediate_steps\": state.get(\"intermediate_steps\", []),\n",
    "        }\n",
    "\n",
    "\n",
    "# StateGraph ìƒì„±\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "\n",
    "# ì‹œì‘ì ê³¼ ì¢…ë£Œì  ì„¤ì •\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.add_edge(\"agent\", END)\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "smart_agent = graph.compile()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "if __name__ == \"__main__\":\n",
    "    # ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´\n",
    "    print(\"=== ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ ===\")\n",
    "    result1 = smart_agent.invoke(\n",
    "        {\n",
    "            \"input\": \"ì„œìš¸ì˜ ì¸êµ¬ëŠ”?\",\n",
    "            \"chat_history\": [],\n",
    "            \"agent_outcome\": \"\",\n",
    "            \"intermediate_steps\": [],\n",
    "        }\n",
    "    )\n",
    "    print(result1[\"agent_outcome\"])\n",
    "\n",
    "    print(\"\\n=== ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì •ë³´ (LLM Fallback) ===\")\n",
    "    result2 = smart_agent.invoke(\n",
    "        {\n",
    "            \"input\": \"ëŒ€êµ¬ì˜ ì¸êµ¬ëŠ”?\",\n",
    "            \"chat_history\": [],\n",
    "            \"agent_outcome\": \"\",\n",
    "            \"intermediate_steps\": [],\n",
    "        }\n",
    "    )\n",
    "    print(result2[\"agent_outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab4982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
