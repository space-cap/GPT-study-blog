{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1680f910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f8c7dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-cb95d90c-1834-491b-9cef-f4adb2b3c69b', 'json_data': {'messages': [{'content': 'Îã§Ïùå ÏßàÎ¨∏Ïóê ÎãµÌï¥Ï£ºÏÑ∏Ïöî: ÏÑúÏö∏Ïùò Ïù∏Íµ¨Îäî?', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002868ABFC5F0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x000002868A66A060> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002868ABFC500>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ÎèÑÍµ¨ÏóêÏÑú Ï∞æÏùÑ Ïàò ÏûàÎäî Ï†ïÎ≥¥ ===\n",
      "\u001b[1m[values]\u001b[0m {'input': 'ÏÑúÏö∏Ïùò Ïù∏Íµ¨Îäî?', 'chat_history': [], 'agent_outcome': None, 'intermediate_steps': [], 'output': ''}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "‚ùå [DEBUG] Agent Ïã§Ìñâ Ïã§Ìå®: RunnableAgent.plan() got multiple values for argument 'intermediate_steps'\n",
      "üîÑ [DEBUG] Fallback Î™®ÎìúÎ°ú Ï†ÑÌôò\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Jul 2025 13:45:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'ailysfrontier'), (b'openai-processing-ms', b'1625'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'1628'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199983'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'req_bc40a2cc68cffda59a2545aa2ca31690'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'959f105dff88ea15-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 04 Jul 2025 13:45:37 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'ailysfrontier', 'openai-processing-ms': '1625', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '1628', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199983', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '4ms', 'x-request-id': 'req_bc40a2cc68cffda59a2545aa2ca31690', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '959f105dff88ea15-ICN', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_bc40a2cc68cffda59a2545aa2ca31690\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-73d0e83c-cad3-4e47-b446-2c6e782da7c9', 'json_data': {'messages': [{'content': 'Îã§Ïùå ÏßàÎ¨∏Ïóê ÎãµÌï¥Ï£ºÏÑ∏Ïöî: ÎåÄÍµ¨Ïùò Ïù∏Íµ¨Îäî?', 'role': 'user'}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[updates]\u001b[0m {'agent': {'input': 'ÏÑúÏö∏Ïùò Ïù∏Íµ¨Îäî?', 'chat_history': [], 'agent_outcome': None, 'intermediate_steps': [], 'output': 'ü§ñ ÎèÑÍµ¨ÏóêÏÑú Ï†ïÎ≥¥Î•º Ï∞æÏßÄ Î™ªÌï¥ ÏùºÎ∞ò ÏßÄÏãùÏúºÎ°ú ÎãµÎ≥ÄÌï©ÎãàÎã§:\\n2023ÎÖÑ Í∏∞Ï§ÄÏúºÎ°ú ÏÑúÏö∏Ïùò Ïù∏Íµ¨Îäî ÏïΩ 9Î∞±Îßå Î™Ö Ï†ïÎèÑÏûÖÎãàÎã§. ÌïòÏßÄÎßå Ïù∏Íµ¨Îäî ÏßÄÏÜçÏ†ÅÏúºÎ°ú Î≥ÄÎèôÌï† Ïàò ÏûàÏúºÎØÄÎ°ú, ÏµúÏã† ÌÜµÍ≥Ñ ÏûêÎ£åÎ•º ÌôïÏù∏ÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§. ÏÑúÏö∏ÌäπÎ≥ÑÏãúÏ≤≠Ïù¥ÎÇò ÌÜµÍ≥ÑÏ≤≠Ïùò Í≥µÏãù ÏõπÏÇ¨Ïù¥Ìä∏ÏóêÏÑú ÏµúÏã† Ï†ïÎ≥¥Î•º ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.'}}\n",
      "\u001b[1m[values]\u001b[0m {'input': 'ÏÑúÏö∏Ïùò Ïù∏Íµ¨Îäî?', 'chat_history': [], 'agent_outcome': None, 'intermediate_steps': [], 'output': 'ü§ñ ÎèÑÍµ¨ÏóêÏÑú Ï†ïÎ≥¥Î•º Ï∞æÏßÄ Î™ªÌï¥ ÏùºÎ∞ò ÏßÄÏãùÏúºÎ°ú ÎãµÎ≥ÄÌï©ÎãàÎã§:\\n2023ÎÖÑ Í∏∞Ï§ÄÏúºÎ°ú ÏÑúÏö∏Ïùò Ïù∏Íµ¨Îäî ÏïΩ 9Î∞±Îßå Î™Ö Ï†ïÎèÑÏûÖÎãàÎã§. ÌïòÏßÄÎßå Ïù∏Íµ¨Îäî ÏßÄÏÜçÏ†ÅÏúºÎ°ú Î≥ÄÎèôÌï† Ïàò ÏûàÏúºÎØÄÎ°ú, ÏµúÏã† ÌÜµÍ≥Ñ ÏûêÎ£åÎ•º ÌôïÏù∏ÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§. ÏÑúÏö∏ÌäπÎ≥ÑÏãúÏ≤≠Ïù¥ÎÇò ÌÜµÍ≥ÑÏ≤≠Ïùò Í≥µÏãù ÏõπÏÇ¨Ïù¥Ìä∏ÏóêÏÑú ÏµúÏã† Ï†ïÎ≥¥Î•º ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.'}\n",
      "ü§ñ ÎèÑÍµ¨ÏóêÏÑú Ï†ïÎ≥¥Î•º Ï∞æÏßÄ Î™ªÌï¥ ÏùºÎ∞ò ÏßÄÏãùÏúºÎ°ú ÎãµÎ≥ÄÌï©ÎãàÎã§:\n",
      "2023ÎÖÑ Í∏∞Ï§ÄÏúºÎ°ú ÏÑúÏö∏Ïùò Ïù∏Íµ¨Îäî ÏïΩ 9Î∞±Îßå Î™Ö Ï†ïÎèÑÏûÖÎãàÎã§. ÌïòÏßÄÎßå Ïù∏Íµ¨Îäî ÏßÄÏÜçÏ†ÅÏúºÎ°ú Î≥ÄÎèôÌï† Ïàò ÏûàÏúºÎØÄÎ°ú, ÏµúÏã† ÌÜµÍ≥Ñ ÏûêÎ£åÎ•º ÌôïÏù∏ÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§. ÏÑúÏö∏ÌäπÎ≥ÑÏãúÏ≤≠Ïù¥ÎÇò ÌÜµÍ≥ÑÏ≤≠Ïùò Í≥µÏãù ÏõπÏÇ¨Ïù¥Ìä∏ÏóêÏÑú ÏµúÏã† Ï†ïÎ≥¥Î•º ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
      "\n",
      "=== ÎèÑÍµ¨ÏóêÏÑú Ï∞æÏùÑ Ïàò ÏóÜÎäî Ï†ïÎ≥¥ (LLM Fallback) ===\n",
      "\u001b[1m[values]\u001b[0m {'input': 'ÎåÄÍµ¨Ïùò Ïù∏Íµ¨Îäî?', 'chat_history': [], 'agent_outcome': None, 'intermediate_steps': [], 'output': ''}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "‚ùå [DEBUG] Agent Ïã§Ìñâ Ïã§Ìå®: RunnableAgent.plan() got multiple values for argument 'intermediate_steps'\n",
      "üîÑ [DEBUG] Fallback Î™®ÎìúÎ°ú Ï†ÑÌôò\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Jul 2025 13:45:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'ailysfrontier'), (b'openai-processing-ms', b'1092'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'1097'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'199983'), (b'x-ratelimit-reset-requests', b'15.201s'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'req_196f69a1216535d1d8bd054a5f1419bd'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'959f10697d31ea15-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 04 Jul 2025 13:45:39 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'ailysfrontier', 'openai-processing-ms': '1092', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '1097', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '199983', 'x-ratelimit-reset-requests': '15.201s', 'x-ratelimit-reset-tokens': '4ms', 'x-request-id': 'req_196f69a1216535d1d8bd054a5f1419bd', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '959f10697d31ea15-ICN', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_196f69a1216535d1d8bd054a5f1419bd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[updates]\u001b[0m {'agent': {'input': 'ÎåÄÍµ¨Ïùò Ïù∏Íµ¨Îäî?', 'chat_history': [], 'agent_outcome': None, 'intermediate_steps': [], 'output': 'ü§ñ ÎèÑÍµ¨ÏóêÏÑú Ï†ïÎ≥¥Î•º Ï∞æÏßÄ Î™ªÌï¥ ÏùºÎ∞ò ÏßÄÏãùÏúºÎ°ú ÎãµÎ≥ÄÌï©ÎãàÎã§:\\nÎåÄÍµ¨Ïùò Ïù∏Íµ¨Îäî 2023ÎÖÑ Í∏∞Ï§ÄÏúºÎ°ú ÏïΩ 240Îßå Î™Ö Ï†ïÎèÑÏûÖÎãàÎã§. ÌïòÏßÄÎßå Ïù∏Íµ¨Îäî ÏãúÍ∞ÑÏù¥ ÏßÄÎÇ®Ïóê Îî∞Îùº Î≥ÄÎèôÌï† Ïàò ÏûàÏúºÎØÄÎ°ú, ÏµúÏã† Ï†ïÎ≥¥Î•º ÌôïÏù∏ÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§. ÎåÄÍµ¨Ïùò Ïù∏Íµ¨Ïóê ÎåÄÌïú Ï†ïÌôïÌïú ÏàòÏπòÎäî ÎåÄÍµ¨Í¥ëÏó≠ÏãúÏ≤≠Ïù¥ÎÇò ÌÜµÍ≥ÑÏ≤≠Ïùò Í≥µÏãù ÏûêÎ£åÎ•º Ï∞∏Í≥†ÌïòÏãúÍ∏∞ Î∞îÎûçÎãàÎã§.'}}\n",
      "\u001b[1m[values]\u001b[0m {'input': 'ÎåÄÍµ¨Ïùò Ïù∏Íµ¨Îäî?', 'chat_history': [], 'agent_outcome': None, 'intermediate_steps': [], 'output': 'ü§ñ ÎèÑÍµ¨ÏóêÏÑú Ï†ïÎ≥¥Î•º Ï∞æÏßÄ Î™ªÌï¥ ÏùºÎ∞ò ÏßÄÏãùÏúºÎ°ú ÎãµÎ≥ÄÌï©ÎãàÎã§:\\nÎåÄÍµ¨Ïùò Ïù∏Íµ¨Îäî 2023ÎÖÑ Í∏∞Ï§ÄÏúºÎ°ú ÏïΩ 240Îßå Î™Ö Ï†ïÎèÑÏûÖÎãàÎã§. ÌïòÏßÄÎßå Ïù∏Íµ¨Îäî ÏãúÍ∞ÑÏù¥ ÏßÄÎÇ®Ïóê Îî∞Îùº Î≥ÄÎèôÌï† Ïàò ÏûàÏúºÎØÄÎ°ú, ÏµúÏã† Ï†ïÎ≥¥Î•º ÌôïÏù∏ÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§. ÎåÄÍµ¨Ïùò Ïù∏Íµ¨Ïóê ÎåÄÌïú Ï†ïÌôïÌïú ÏàòÏπòÎäî ÎåÄÍµ¨Í¥ëÏó≠ÏãúÏ≤≠Ïù¥ÎÇò ÌÜµÍ≥ÑÏ≤≠Ïùò Í≥µÏãù ÏûêÎ£åÎ•º Ï∞∏Í≥†ÌïòÏãúÍ∏∞ Î∞îÎûçÎãàÎã§.'}\n",
      "ü§ñ ÎèÑÍµ¨ÏóêÏÑú Ï†ïÎ≥¥Î•º Ï∞æÏßÄ Î™ªÌï¥ ÏùºÎ∞ò ÏßÄÏãùÏúºÎ°ú ÎãµÎ≥ÄÌï©ÎãàÎã§:\n",
      "ÎåÄÍµ¨Ïùò Ïù∏Íµ¨Îäî 2023ÎÖÑ Í∏∞Ï§ÄÏúºÎ°ú ÏïΩ 240Îßå Î™Ö Ï†ïÎèÑÏûÖÎãàÎã§. ÌïòÏßÄÎßå Ïù∏Íµ¨Îäî ÏãúÍ∞ÑÏù¥ ÏßÄÎÇ®Ïóê Îî∞Îùº Î≥ÄÎèôÌï† Ïàò ÏûàÏúºÎØÄÎ°ú, ÏµúÏã† Ï†ïÎ≥¥Î•º ÌôïÏù∏ÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§. ÎåÄÍµ¨Ïùò Ïù∏Íµ¨Ïóê ÎåÄÌïú Ï†ïÌôïÌïú ÏàòÏπòÎäî ÎåÄÍµ¨Í¥ëÏó≠ÏãúÏ≤≠Ïù¥ÎÇò ÌÜµÍ≥ÑÏ≤≠Ïùò Í≥µÏãù ÏûêÎ£åÎ•º Ï∞∏Í≥†ÌïòÏãúÍ∏∞ Î∞îÎûçÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import Tool, create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "import operator\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# ÌôòÍ≤Ω Î≥ÄÏàò\n",
    "load_dotenv()\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# AgentState Ï†ïÏùò (Ïò¨Î∞îÎ•∏ ÌÉÄÏûÖ ÏßÄÏ†ï)\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[List[BaseMessage], operator.add]\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "    intermediate_steps: Annotated[List[tuple], operator.add]\n",
    "    output: str  # ÏµúÏ¢Ö Ï∂úÎ†•Ïö© ÌïÑÎìú Ï∂îÍ∞Ä\n",
    "\n",
    "\n",
    "# Í∏∞Ï°¥ ÎèÑÍµ¨ Ìï®ÏàòÎì§\n",
    "def add_two(x: str) -> str:\n",
    "    try:\n",
    "        return str(float(x) + 2)\n",
    "    except Exception:\n",
    "        raise ValueError(\"Ïà´ÏûêÎßå ÏûÖÎ†•Ìï¥ Ï£ºÏÑ∏Ïöî.\")\n",
    "\n",
    "\n",
    "def population_lookup(city: str) -> str:\n",
    "    data = {\n",
    "        \"ÏÑúÏö∏\": \"9,733,509Î™Ö (2025ÎÖÑ Í∏∞Ï§Ä)\",\n",
    "        \"Î∂ÄÏÇ∞\": \"3,400,000Î™Ö (2025ÎÖÑ Í∏∞Ï§Ä)\",\n",
    "    }\n",
    "    # ÎåÄÏÜåÎ¨∏Ïûê Íµ¨Î∂Ñ ÏóÜÏù¥ Í≤ÄÏÉâÌïòÎèÑÎ°ù ÏàòÏ†ï\n",
    "    city_lower = city.strip().lower()\n",
    "    for key, value in data.items():\n",
    "        if key.lower() == city_lower:\n",
    "            return value\n",
    "    raise ValueError(f\"{city}Ïùò Ïù∏Íµ¨ Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "\n",
    "# ÎèÑÍµ¨ Î∞è LLM ÏÑ§Ï†ï\n",
    "tools = [\n",
    "    Tool(name=\"AddTwo\", func=add_two, description=\"Ïà´Ïûê ÏûÖÎ†• Ïãú 2Î•º ÎçîÌï©ÎãàÎã§.\"),\n",
    "    Tool(\n",
    "        name=\"PopulationLookup\",\n",
    "        func=population_lookup,\n",
    "        description=\"ÎèÑÏãú Ïù¥Î¶Ñ ÏûÖÎ†• Ïãú Ïù∏Íµ¨ Ï†ïÎ≥¥Î•º Î∞òÌôòÌï©ÎãàÎã§.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", temperature=0.1, openai_api_key=OPEN_API_KEY, verbose=True\n",
    ")\n",
    "\n",
    "# ReAct ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ§Ï†ï\n",
    "react_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "ÎãπÏã†ÏùÄ ÎèÑÏõÄÏù¥ ÎêòÎäî Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. Ï£ºÏñ¥ÏßÑ ÎèÑÍµ¨Î•º ÏÇ¨Ïö©Ìï¥ÏÑú ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî.\n",
    "\n",
    "ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨:\n",
    "{tools}\n",
    "\n",
    "ÎèÑÍµ¨ Ïù¥Î¶Ñ: {tool_names}\n",
    "\n",
    "Îã§Ïùå ÌòïÏãùÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî:\n",
    "\n",
    "Question: ÎãµÎ≥ÄÌï¥Ïïº Ìï† ÏûÖÎ†• ÏßàÎ¨∏\n",
    "Thought: Î¨¥ÏóáÏùÑ Ìï¥Ïïº Ìï†ÏßÄ ÏÉùÍ∞ÅÌï¥Î≥¥ÏÑ∏Ïöî\n",
    "Action: ÏàòÌñâÌï† Ïï°ÏÖò, [{tool_names}] Ï§ë ÌïòÎÇòÏó¨Ïïº Ìï©ÎãàÎã§\n",
    "Action Input: Ïï°ÏÖòÏóê ÎåÄÌïú ÏûÖÎ†•\n",
    "Observation: Ïï°ÏÖòÏùò Í≤∞Í≥º\n",
    "... (Ïù¥ Thought/Action/Action Input/ObservationÏùÑ ÌïÑÏöîÌïú ÎßåÌÅº Î∞òÎ≥µ)\n",
    "Thought: Ïù¥Ï†ú ÏµúÏ¢Ö ÎãµÎ≥ÄÏùÑ ÏïåÏïòÏäµÎãàÎã§\n",
    "Final Answer: ÏõêÎûò ÏûÖÎ†• ÏßàÎ¨∏Ïóê ÎåÄÌïú ÏµúÏ¢Ö ÎãµÎ≥Ä\n",
    "\n",
    "ÏãúÏûëÌïòÏÑ∏Ïöî!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ReAct AgentÏôÄ AgentExecutor ÏÉùÏÑ±\n",
    "react_agent = create_react_agent(llm, tools, react_prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=react_agent,\n",
    "    tools=tools,\n",
    "    verbose=True,  # Agent Ïã§Ìñâ Í≥ºÏ†ï Ï∂úÎ†•\n",
    "    return_intermediate_steps=True,  # Ï§ëÍ∞Ñ Îã®Í≥Ñ Î∞òÌôò\n",
    "    max_iterations=10,  # ÎîîÎ≤ÑÍπÖÏö© ÏµúÎåÄ Î∞òÎ≥µ Ï†úÌïú\n",
    ")\n",
    "\n",
    "\n",
    "# Agent ÎÖ∏Îìú Ìï®Ïàò (ÏàòÏ†ïÎê®)\n",
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    try:\n",
    "        # AgentExecutor ÏÇ¨Ïö©ÌïòÏó¨ Ïã§Ìñâ\n",
    "        result = agent_executor.invoke(\n",
    "            {\n",
    "                \"input\": state[\"input\"],\n",
    "                \"chat_history\": state.get(\"chat_history\", []),\n",
    "                \"intermediate_steps\": state.get(\"intermediate_steps\", []),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ [DEBUG] Agent Ïã§Ìñâ ÏÑ±Í≥µ\")\n",
    "        print(f\"üìä [DEBUG] Ï§ëÍ∞Ñ Îã®Í≥Ñ Ïàò: {len(result.get('intermediate_steps', []))}\")\n",
    "\n",
    "        return {\n",
    "            \"input\": state[\"input\"],\n",
    "            \"chat_history\": state.get(\"chat_history\", []),\n",
    "            \"agent_outcome\": None,  # AgentExecutorÎäî Ïù¥ÎØ∏ ÏôÑÎ£åÎêú Í≤∞Í≥ºÎ•º Î∞òÌôò\n",
    "            \"intermediate_steps\": result.get(\"intermediate_steps\", []),\n",
    "            \"output\": result[\"output\"],  # ÏµúÏ¢Ö Í≤∞Í≥ºÎ•º output ÌïÑÎìúÏóê Ï†ÄÏû•\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå [DEBUG] Agent Ïã§Ìñâ Ïã§Ìå®: {str(e)}\")\n",
    "        print(f\"üîÑ [DEBUG] Fallback Î™®ÎìúÎ°ú Ï†ÑÌôò\")\n",
    "\n",
    "        # Fallback Ï≤òÎ¶¨\n",
    "        response = llm.invoke(f\"Îã§Ïùå ÏßàÎ¨∏Ïóê ÎãµÌï¥Ï£ºÏÑ∏Ïöî: {state['input']}\")\n",
    "        fallback_result = f\"ü§ñ ÎèÑÍµ¨ÏóêÏÑú Ï†ïÎ≥¥Î•º Ï∞æÏßÄ Î™ªÌï¥ ÏùºÎ∞ò ÏßÄÏãùÏúºÎ°ú ÎãµÎ≥ÄÌï©ÎãàÎã§:\\n{response.content}\"\n",
    "\n",
    "        return {\n",
    "            \"input\": state[\"input\"],\n",
    "            \"chat_history\": state.get(\"chat_history\", []),\n",
    "            \"agent_outcome\": None,\n",
    "            \"intermediate_steps\": state.get(\"intermediate_steps\", []),\n",
    "            \"output\": fallback_result,\n",
    "        }\n",
    "\n",
    "\n",
    "# StateGraph ÏÉùÏÑ±\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "\n",
    "# ÏãúÏûëÏ†êÍ≥º Ï¢ÖÎ£åÏ†ê ÏÑ§Ï†ï\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.add_edge(\"agent\", END)\n",
    "\n",
    "# Í∑∏ÎûòÌîÑ Ïª¥ÌååÏùº\n",
    "smart_agent = graph.compile(debug=True)\n",
    "\n",
    "# ÌÖåÏä§Ìä∏\n",
    "if __name__ == \"__main__\":\n",
    "    # ÎèÑÍµ¨ÏóêÏÑú Ï∞æÏùÑ Ïàò ÏûàÎäî Ï†ïÎ≥¥\n",
    "    print(\"=== ÎèÑÍµ¨ÏóêÏÑú Ï∞æÏùÑ Ïàò ÏûàÎäî Ï†ïÎ≥¥ ===\")\n",
    "    result1 = smart_agent.invoke(\n",
    "        {\n",
    "            \"input\": \"ÏÑúÏö∏Ïùò Ïù∏Íµ¨Îäî?\",\n",
    "            \"chat_history\": [],\n",
    "            \"agent_outcome\": None,\n",
    "            \"intermediate_steps\": [],\n",
    "            \"output\": \"\",\n",
    "        }\n",
    "    )\n",
    "    print(result1[\"output\"])\n",
    "\n",
    "    print(\"\\n=== ÎèÑÍµ¨ÏóêÏÑú Ï∞æÏùÑ Ïàò ÏóÜÎäî Ï†ïÎ≥¥ (LLM Fallback) ===\")\n",
    "    result2 = smart_agent.invoke(\n",
    "        {\n",
    "            \"input\": \"ÎåÄÍµ¨Ïùò Ïù∏Íµ¨Îäî?\",\n",
    "            \"chat_history\": [],\n",
    "            \"agent_outcome\": None,\n",
    "            \"intermediate_steps\": [],\n",
    "            \"output\": \"\",\n",
    "        }\n",
    "    )\n",
    "    print(result2[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab4982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
