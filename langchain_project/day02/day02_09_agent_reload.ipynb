{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e53f335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aac6fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ ===\n",
      "ì„œìš¸ì˜ ì¸êµ¬ëŠ” 9,733,509ëª…ì…ë‹ˆë‹¤ (2025ë…„ ê¸°ì¤€).\n",
      "\n",
      "========================================\n",
      "\n",
      "=== ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì •ë³´ (LLM Fallback) ===\n",
      "ëŒ€êµ¬ì˜ ì¸êµ¬ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œìš¸ì´ë‚˜ ë¶€ì‚°ì˜ ì¸êµ¬ ì •ë³´ë¥¼ ìš”ì²­í•´ ì£¼ì‹œë©´ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import Tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, ToolMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# --- í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ---\n",
    "load_dotenv()\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# --- ê¸°ì¡´ ë„êµ¬ í•¨ìˆ˜ë“¤ (ë³€ê²½ ì—†ìŒ) ---\n",
    "def add_two(x: str) -> str:\n",
    "    \"\"\"ìˆ«ì ì…ë ¥ ì‹œ 2ë¥¼ ë”í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        return str(float(x) + 2)\n",
    "    except Exception:\n",
    "        raise ValueError(\"ìˆ«ìë§Œ ì…ë ¥í•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "def population_lookup(city: str) -> str:\n",
    "    \"\"\"ë„ì‹œ ì´ë¦„ ì…ë ¥ ì‹œ ì¸êµ¬ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    data = {\n",
    "        \"ì„œìš¸\": \"9,733,509ëª… (2025ë…„ ê¸°ì¤€)\",\n",
    "        \"ë¶€ì‚°\": \"3,400,000ëª… (2025ë…„ ê¸°ì¤€)\",\n",
    "    }\n",
    "    # ë„ì‹œ ì´ë¦„ì˜ ì–‘ìª½ ê³µë°±ì„ ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€\n",
    "    result = data.get(city.strip().lower())\n",
    "    if result:\n",
    "        return result\n",
    "    else:\n",
    "        raise ValueError(f\"{city}ì˜ ì¸êµ¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# --- ë„êµ¬ ë° LLM ì„¤ì • (ë³€ê²½ ì—†ìŒ) ---\n",
    "tools = [\n",
    "    Tool(name=\"AddTwo\", func=add_two, description=\"ìˆ«ì ì…ë ¥ ì‹œ 2ë¥¼ ë”í•©ë‹ˆë‹¤.\"),\n",
    "    Tool(\n",
    "        name=\"PopulationLookup\",\n",
    "        func=population_lookup,\n",
    "        description=\"ë„ì‹œ ì´ë¦„ ì…ë ¥ ì‹œ ì¸êµ¬ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ë„ì‹œ ì´ë¦„ì€ 'ì„œìš¸', 'ë¶€ì‚°'ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=OPEN_API_KEY)\n",
    "\n",
    "\n",
    "# --- LangGraph ìƒíƒœ ì •ì˜ ---\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Agentì˜ ìƒíƒœë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë©”ì‹œì§€ ëª©ë¡ì„ í¬í•¨í•˜ë©°, ê° ë‹¨ê³„ì—ì„œ ë©”ì‹œì§€ê°€ ì¶”ê°€ë©ë‹ˆë‹¤.\n",
    "\n",
    "    Attributes:\n",
    "        messages (Sequence[BaseMessage]): HumanMessage, AIMessage, ToolMessage ë“± ëŒ€í™” ê¸°ë¡\n",
    "    \"\"\"\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "\n",
    "# --- LangGraph ê¸°ë°˜ ReAct Agent ìƒì„± ---\n",
    "# LangGraphì— ì‚¬ì „ ë¹Œë“œëœ create_react_agentë¥¼ ì‚¬ìš©í•˜ì—¬ Agent ë…¸ë“œë¥¼ ê°„ë‹¨íˆ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# ì´ ë…¸ë“œëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ReAct í”„ë¡¬í”„íŠ¸, LLM, ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "react_agent = create_react_agent(llm, tools)\n",
    "\n",
    "\n",
    "# --- Fallback í•¨ìˆ˜ ì •ì˜ ---\n",
    "# ë„êµ¬ë¡œ í•´ê²°í•  ìˆ˜ ì—†ëŠ” ê²½ìš° í˜¸ì¶œë  í•¨ìˆ˜\n",
    "def create_llm_fallback():\n",
    "    def llm_fallback(state: AgentState) -> dict:\n",
    "        \"\"\"\n",
    "        ì´ˆê¸° ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ LLMì´ ì§ì ‘ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” Fallback ë¡œì§\n",
    "        \"\"\"\n",
    "        # ê°€ì¥ ì²˜ìŒì˜ ì‚¬ìš©ì ì§ˆë¬¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "        user_input = state[\"messages\"][0].content\n",
    "\n",
    "        # LLMì„ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±\n",
    "        prompt = f\"ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”: {user_input}\"\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        # Fallbackì´ ì‹¤í–‰ë˜ì—ˆìŒì„ ì•Œë¦¬ëŠ” ë©”ì‹œì§€ì™€ í•¨ê»˜ ê²°ê³¼ ë°˜í™˜\n",
    "        fallback_message = f\"ğŸ¤– ë„êµ¬ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤:\\n{response.content}\"\n",
    "\n",
    "        return {\"messages\": [AIMessage(content=fallback_message)]}\n",
    "\n",
    "    return llm_fallback\n",
    "\n",
    "\n",
    "# --- Graph ìƒì„± ë° ì»´íŒŒì¼ ---\n",
    "# StateGraphì— AgentStateë¥¼ ì •ì˜í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# ë…¸ë“œë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€í•©ë‹ˆë‹¤. 'agent'ëŠ” ì¶”ë¡ , 'action'ì€ ë„êµ¬ ì‹¤í–‰ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.\n",
    "# `create_react_agent`ëŠ” Agent ë¡œì§ê³¼ Tool ì‹¤í–‰ ë¡œì§ì„ í•©ì¹œ ë…¸ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "graph.add_node(\"agent\", react_agent)\n",
    "\n",
    "# ê·¸ë˜í”„ì˜ ì‹œì‘ì ì„ 'agent' ë…¸ë“œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "graph.set_entry_point(\"agent\")\n",
    "\n",
    "# ê·¸ë˜í”„ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ë³„í•œ ë…¸ë“œ ì´ë¦„ì¸ ENDë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# 'agent' ë…¸ë“œì—ì„œ ë„êµ¬ í˜¸ì¶œì´ í•„ìš” ì—†ìœ¼ë©´(ì¦‰, ìµœì¢… ë‹µë³€ì´ ìƒì„±ë˜ë©´) ê·¸ë˜í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\n",
    "graph.add_edge(\"agent\", END)\n",
    "\n",
    "# Fallback í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "llm_fallback_runnable = RunnableLambda(create_llm_fallback())\n",
    "\n",
    "# ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•˜ê³ , fallback ë¡œì§ì„ ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "# agent(ë‚´ë¶€ì ìœ¼ë¡œ tool í¬í•¨) ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ llm_fallback_runnableì´ ì‹¤í–‰ë©ë‹ˆë‹¤.\n",
    "smart_agent = graph.compile().with_fallbacks(fallbacks=[llm_fallback_runnable])\n",
    "\n",
    "# --- í…ŒìŠ¤íŠ¸ ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ ===\")\n",
    "    # ì´ˆê¸° ìƒíƒœë¥¼ HumanMessageì™€ í•¨ê»˜ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì œê³µí•©ë‹ˆë‹¤.\n",
    "    inputs1 = {\"messages\": [HumanMessage(content=\"ì„œìš¸ì˜ ì¸êµ¬ëŠ”?\")]}\n",
    "    result1 = smart_agent.invoke(inputs1)\n",
    "    # ìµœì¢… ê²°ê³¼ëŠ” 'messages' ë¦¬ìŠ¤íŠ¸ì˜ ë§ˆì§€ë§‰ AIMessageì— ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.\n",
    "    print(result1[\"messages\"][-1].content)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 40 + \"\\n\")\n",
    "\n",
    "    print(\"=== ë„êµ¬ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì •ë³´ (LLM Fallback) ===\")\n",
    "    inputs2 = {\"messages\": [HumanMessage(content=\"ëŒ€êµ¬ì˜ ì¸êµ¬ëŠ”?\")]}\n",
    "    result2 = smart_agent.invoke(inputs2)\n",
    "    print(result2[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c84154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
