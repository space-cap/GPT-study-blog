{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8353d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22663aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 1. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "# OpenAIì˜ text-embedding-ada-002 ëª¨ë¸ ì‚¬ìš©\n",
    "# ê¸°ë³¸ 1536ì°¨ì› : 1536 Ã— 4ë°”ì´íŠ¸ = 6.1KB per ë¬¸ì„œ\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 1024ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ: 512 Ã— 4ë°”ì´íŠ¸ = 2.0KB per ë¬¸ì„œ (ì•½ 67% ì ˆì•½)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d47b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Tesseract ì‹¤í–‰ íŒŒì¼ ê²½ë¡œë¥¼ ì§ì ‘ ì§€ì •\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "\n",
    "def load_and_process_document_with_ocr(file_path: str):\n",
    "    \"\"\"\n",
    "    PDF ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” í˜ì´ì§€ëŠ” ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ OCR ìˆ˜í–‰.\n",
    "    ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ LangChain Documentë¡œ ë³€í™˜ í›„ ì²­í¬ ë¶„í• í•˜ì—¬ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    if ext != \".pdf\":\n",
    "        raise ValueError(\"í˜„ì¬ëŠ” PDF íŒŒì¼ë§Œ ì§€ì›í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    raw_docs = []\n",
    "    pdf = fitz.open(file_path)\n",
    "\n",
    "    for page_idx in range(len(pdf)):\n",
    "        page = pdf.load_page(page_idx)\n",
    "        text = page.get_text(\"text\")\n",
    "\n",
    "        if not text.strip():\n",
    "            # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ OCR ìˆ˜í–‰\n",
    "            print(f\"í˜ì´ì§€ {page_idx + 1}: í…ìŠ¤íŠ¸ê°€ ì—†ì–´ OCR ìˆ˜í–‰ ì¤‘...\")\n",
    "            pix = page.get_pixmap()\n",
    "            img_bytes = pix.tobytes(\"png\")\n",
    "            img = Image.open(io.BytesIO(img_bytes))\n",
    "            ocr_text = pytesseract.image_to_string(img, lang=\"kor+eng\")\n",
    "            text = ocr_text\n",
    "\n",
    "        if text.strip():\n",
    "            raw_docs.append(\n",
    "                Document(\n",
    "                    page_content=text,\n",
    "                    metadata={\"source\": file_path, \"page\": page_idx + 1},\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(f\"í˜ì´ì§€ {page_idx + 1}: OCR í›„ì—ë„ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    pdf.close()\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ë¶„í• \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_documents(raw_docs)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bcde86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²­í¬ ê°œìˆ˜: 5\n"
     ]
    }
   ],
   "source": [
    "chunks = load_and_process_document_with_ocr(\"sample_02_ocr.pdf\")\n",
    "print(f\"ì²­í¬ ê°œìˆ˜: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "226595ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.vectorstores.chroma.Chroma"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks, embedding=embeddings, persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "type(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd4f06d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ìƒ‰ ì¿¼ë¦¬: ì²­ë…„ì¼ìë¦¬ê°€ ë­”ê°€ìš”?\n",
      "==================================================\n",
      "1. ì²­ë…„ì •ì±… ì •ë³´\n",
      "â–  ì •ì±… ê¸°ë³¸ì •ë³´\n",
      "ì •ì±…ëª…\n",
      "ì²­ë…„ì¼ìë¦¬ ë„ì•½ì¥ë ¤ê¸ˆ\n",
      "ì •ì±…ì†Œê°œ\n",
      "ê¸°ì—…ì˜ ì²­ë…„ê³ ìš© í™•ëŒ€ë¥¼ ì§€ì›í•˜ê³ ï¼Œì·¨ì—…ì• ë¡œ ì²­ë…„ì˜ ì·¨ì—…ì„ ì´‰ì§„í•¨ìœ¼ë¡œì¨ï¼Œ \n",
      ":glë…„ê³ \"ìš© í™œ'ì„±íˆâ– â– ë¥¼ â– ëª©:ì ìœ¼â– ë¡œ. íˆâ– â– ëŠ” :ì•¡\n",
      "O ìœ ë‚ ã€ã€5ì¸ Sì‹±\"êµ´ì†Œê¸°ì—…ì–´ã€‚ì¶”å£«ì• ë¡œì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ì˜¹í•˜ê³ \n",
      "6ê°œì›” ì´ìƒ ê³ ìš©ìœ ì§€ì‹œ ê¸°ì—…ì§€ì›ê¸ˆ ì§€ê¸‰\n",
      "O ï¼ˆìœ í˜•nï¼‰ ë¹ˆì¼ìë¦¬ ì—…ì¢… ì¤‘ì†Œê¸°ì—…ì—ì„œ ì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ìš© í›„ 6ê°œì›” \n",
      "ì´ìƒ ê³ ìš©ìœ ì§€ì‹œ ê¸°ì—…ì§€ì›ê¸ˆ ì§€ê¸‰í•˜ê³ ï¼Œí•´ë‹¹ ê¸°ì—…ì—ì„œ 18ê°œì›” ì´ìƒ ì¬ì§í•œ \n",
      "ì²­ë…„ì—ê²Œ ì²­ë…„ ì¥ê¸°ê·¼ì† ì¸ì„¼í‹°ë¸Œ ì§€ê¸‰\n",
      "ì •ì±… ë¶„ì•¼\n",
      "ì¼ìë¦¬\n",
      "ì§€ì› ë‚´ìš©\n",
      "o ï¼ˆìœ í˜• 1ï¼‰5ì¸ ì´ìƒ ìš°ì„ ì§€ì›ëŒ€ìƒê¸°ì—…ì—ì„œ ì·¨ì—…ì• ë¡œì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ \n",
      "ìš©ì‹œ 1ë…„ê°„ ìµœëŒ€ 720ë§Œì› ì§€ì›<br> * ì·¨ì—…ì• ë¡œì²­ë…„: ë§Œ 15-34ì„¸ì˜ 4ê°œì›” \n",
      "ì´ìƒ ì‹¤ì—…ï¼Œê³ ì¡¸ ì´í•˜ ì²­ë…„ ë“± <br> â—¦ï¼ˆìœ í˜• nï¼‰ 5ì¸ ì´ìƒì˜ ì œì¡°ì—… ë“± ë¹ˆì¼ì \n",
      "ë¦¬ ì—…ì¢… ì¤‘ì†Œê¸°ì—…ì—ì„œ ì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ìš©ì‹œ 1ë…„ê°„ ìµœëŒ€ 720ë§Œì› ì§€ì› \n",
      "ë° í•´ë‹¹ ê¸°ì—…ì— ì·¨ì—… í›„ 18ê°œì›” ì´ìƒ ì¬ì§í•œ ì²­ë…„ ê·¼ë¡œìì—ê²Œ ìµœëŒ€ 480ë§Œì› \n",
      "å¤©Iì›ï¼ˆ18.24ê°œì›”å¤§åœ ê° ìƒ0ë§Œì›ï¼‰\n",
      "ì‚¬ì—… ìš´ì˜ ê¸°ê°„\n",
      "2025ë…„ 1ì›” 1ì¼ ã€œ 2025ë…„ 12ì›” 31ì¼\n",
      "ì‚¬ì—… ì‹ ì²­ ê¸°ê°„\n",
      "2025ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 12ì›” 31ì¼<br/>\n",
      "ì§€ì› ê·œëª¨ï¼ˆëª…ï¼‰\n",
      "ì œí•œ ì—†ìŒ\n",
      "ë¹„ê³ \n",
      "1í˜ì´ì§€\n",
      "------------------------------\n",
      "2. ì •ì±… ê¸°ë³¸ ì •ë³´\n",
      "\n",
      "ì •ì±…ëª…\n",
      "\n",
      "ì •ì±…ì†Œê°œ\n",
      "\n",
      "ì •ì±… ë¶„ì•¼\n",
      "\n",
      "ì§€ì› ë‚´ìš©\n",
      "\n",
      "ì‚¬ì—… ìš´ì˜ ê¸°ê°„\n",
      "ì‚¬ì—… ì‹ ì²­ ê¸°ê°„\n",
      "ì§€ì› ê·œëª¨(ëª…)\n",
      "\n",
      "ë¹„ê³ \n",
      "\n",
      "ì²­ë…„ì •ì±… ì •ë³´\n",
      "\n",
      "ì²¨ë…„ì¼ìë¦¬ ë„ì•½ì¥ë ¤ê¸ˆ\n",
      "\n",
      "ê¸°ì—…ì˜ ì²­ë…„ê³ ìš© SHS ì§€ì›í•˜ê³ , ì·¨ì—…ì• ë¡œ ì²­ë…„ì˜ ì·¨ì—…ì„ ì´‰ì§„í•¨ìœ¼ë¡œì¨,\n",
      "ADS í™œì„±í™”ë¥¼ ëª©ì ìœ¼ë¡œ í•˜ëŠ” ì •ì±…\n",
      "\n",
      "0 (ìœ í˜• 1) 5ì¸ ì´ìƒ ì¤‘ì†Œê¸°ì—…ì—ì„œ ì·¨ì—…ì• ë¡œì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ìš©í•˜ê³ \n",
      "oR ì´ìƒ ê³ ìš©ìœ ì§€ì‹œ ê¸°ì—…ì§€ì›ê¸ˆ ì§€ê¸‰\n",
      "\n",
      "ã…‡ (ìœ í˜•) ë¹ˆì¼ìë¦¬ ì—…ì¢… ì¤‘ì†Œê¸°ì—…ì—ì„œ ì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ìš© í›„ 6ê°œì›”\n",
      "ì´ìƒ ê³ ìš©ìœ ì§€ì‹œ ê¸°ì—…ì§€ì›ê¸ˆ ì§€ê¸‰í•˜ê³ , í•´ë‹¹ ê¸°ì—…ì—ì„œ 18ê°œì›” ì´ìƒ ì¬ì§í•œ\n",
      "ì²¨ë…„ì—ê²Œ ì²­ë…„ì¥ê¸°ê·¼ì†ì¸ì„¼í‹°ë¸Œ ì§€ê¸‰\n",
      "\n",
      "ì¼ìë¦¬\n",
      "\n",
      "Â© (ìœ í˜• 1) 5ì¸ ì´ìƒ ìš°ì„ ì§€ì›ëŒ€ìƒê¸°ì—…ì—ì„œ ì·¨ì—…ì• ë¡œì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„\n",
      "\n",
      "ìš©ì‹œ 1ë…„ê°„ ìµœëŒ€ 720ë§Œì› ì§€ì›<6[> *ì·¨ì—…ì• ë¡œì •ë…„: ë§Œ 15-34ì„¸ì˜ 4ê°œì›”\n",
      "ì´ìƒ ì‹¤ì—…, ê³ ì¡¸ ì´í•˜ ì •ë…„ ë“± <br> ã…‡(ìœ í˜•2) 5ì¸ ì´ìƒì˜ ì œì¡°ì—… ë“± ë¹ˆì¼ì\n",
      "\n",
      "ë¦¬ ì—…ì¢… ì¤‘ì†Œê¸°ì—…ì—ì„œ BAS ì •ê·œì§ìœ¼ë¡œ ì±„ìš©ì‹œ 1ë…„ê°„ ìµœëŒ€ 720ë§Œì› ì§€ì›\n",
      "\n",
      "ë° í•´ë‹¹ ê¸°ì—…ì— ì·¨ì—… í›„ 18ê°œì›” ì´ìƒ ì¬ì§í•œ ì²­ë…„ ê·¼ë¡œìì—ê²Œ ìµœëŒ€ 480ë§Œì›\n",
      "\n",
      "ì§€ì›(18.24ê°œì›”ì°¨ ê° 240ë§Œì›)\n",
      "\n",
      "2025ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 128 31ì¼\n",
      "2025ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 12ì›” 31ì¼<61/>\n",
      "\n",
      "ì œí•œì—†ìŒ\n",
      "\n",
      "1í˜ì´ì§€\n",
      "------------------------------\n",
      "3. â–  ì‹ ì²­ ìê²©\n",
      "ì—°ë ¹\n",
      "ë§Œ 15ì„¸ã€œë§Œ 34ì„¸\n",
      "ê±°ì£¼ì§€ ë° ì†Œë“\n",
      "ï¼»ê±°ì£¼ì§€ï¼½\n",
      "ì „êµ­\n",
      "ï¼»ì†Œë“ï¼½\n",
      "ï¼ˆê¸°ì—…ï¼‰ë§¤ì¶œì•¡ ê¸°ì¤€ ï¼ˆì²­ë…„ï¼‰í•´ë‹¹ì—†ìŒ\n",
      "í•™ ë ¥\n",
      "ì œí•œ ì—†ìŒ\n",
      "ì „ê³µ\n",
      "ì œí•œ ì—†ìŒ\n",
      "ì·¨ì—… ìƒíƒœ\n",
      "ì¬ì§ì\n",
      "íŠ¹í™” ë¶„ì•¼\n",
      "ì œí•œ ì—†ìŒ\n",
      "2í˜ì´ì§€\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. ê¸°ë³¸ ìœ ì‚¬ë„ ê²€ìƒ‰\n",
    "def search_similar_documents(query, k=3):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜\n",
    "\n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰í•  ì¿¼ë¦¬\n",
    "        k (int): ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜\n",
    "\n",
    "    Returns:\n",
    "        list: ìœ ì‚¬í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    similar_docs = vectorstore.similarity_search(query=query, k=k)  # ìƒìœ„ kê°œ ë¬¸ì„œ ë°˜í™˜\n",
    "\n",
    "    return similar_docs\n",
    "\n",
    "\n",
    "# ê²€ìƒ‰ ì‹¤í–‰\n",
    "query = \"ì²­ë…„ì¼ìë¦¬ê°€ ë­”ê°€ìš”?\"\n",
    "results = search_similar_documents(query)\n",
    "\n",
    "print(f\"ê²€ìƒ‰ ì¿¼ë¦¬: {query}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca3fb6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ìƒ‰ ì¿¼ë¦¬: ì²­ë…„ì •ì±…ì´ ë­”ê°€ìš”?\n",
      "==================================================\n",
      "1. ìœ ì‚¬ë„ ì ìˆ˜: 1.2781\n",
      "   ë‚´ìš©: ì²­ë…„ì •ì±… ì •ë³´\n",
      "â–  ì •ì±… ê¸°ë³¸ì •ë³´\n",
      "ì •ì±…ëª…\n",
      "ì²­ë…„ì¼ìë¦¬ ë„ì•½ì¥ë ¤ê¸ˆ\n",
      "ì •ì±…ì†Œê°œ\n",
      "ê¸°ì—…ì˜ ì²­ë…„ê³ ìš© í™•ëŒ€ë¥¼ ì§€ì›í•˜ê³ ï¼Œì·¨ì—…ì• ë¡œ ì²­ë…„ì˜ ì·¨ì—…ì„ ì´‰ì§„í•¨ìœ¼ë¡œì¨ï¼Œ \n",
      ":glë…„ê³ \"ìš© í™œ'ì„±íˆâ– â– ë¥¼ â– ëª©:ì ìœ¼â– ë¡œ. íˆâ– â– ëŠ” :ì•¡\n",
      "O ìœ ë‚ ã€ã€5ì¸ Sì‹±\"êµ´ì†Œê¸°ì—…ì–´ã€‚ì¶”å£«ì• ë¡œì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ì˜¹í•˜ê³ \n",
      "6ê°œì›” ì´ìƒ ê³ ìš©ìœ ì§€ì‹œ ê¸°ì—…ì§€ì›ê¸ˆ ì§€ê¸‰\n",
      "O ï¼ˆìœ í˜•nï¼‰ ë¹ˆì¼ìë¦¬ ì—…ì¢… ì¤‘ì†Œê¸°ì—…ì—ì„œ ì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ìš© í›„ 6ê°œì›” \n",
      "ì´ìƒ ê³ ìš©ìœ ì§€ì‹œ ê¸°ì—…ì§€ì›ê¸ˆ ì§€ê¸‰í•˜ê³ ï¼Œí•´ë‹¹ ê¸°ì—…ì—ì„œ 18ê°œì›” ì´ìƒ ì¬ì§í•œ \n",
      "ì²­ë…„ì—ê²Œ ì²­ë…„ ì¥ê¸°ê·¼ì† ì¸ì„¼í‹°ë¸Œ ì§€ê¸‰\n",
      "ì •ì±… ë¶„ì•¼\n",
      "ì¼ìë¦¬\n",
      "ì§€ì› ë‚´ìš©\n",
      "o ï¼ˆìœ í˜• 1ï¼‰5ì¸ ì´ìƒ ìš°ì„ ì§€ì›ëŒ€ìƒê¸°ì—…ì—ì„œ ì·¨ì—…ì• ë¡œì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ \n",
      "ìš©ì‹œ 1ë…„ê°„ ìµœëŒ€ 720ë§Œì› ì§€ì›<br> * ì·¨ì—…ì• ë¡œì²­ë…„: ë§Œ 15-34ì„¸ì˜ 4ê°œì›” \n",
      "ì´ìƒ ì‹¤ì—…ï¼Œê³ ì¡¸ ì´í•˜ ì²­ë…„ ë“± <br> â—¦ï¼ˆìœ í˜• nï¼‰ 5ì¸ ì´ìƒì˜ ì œì¡°ì—… ë“± ë¹ˆì¼ì \n",
      "ë¦¬ ì—…ì¢… ì¤‘ì†Œê¸°ì—…ì—ì„œ ì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ìš©ì‹œ 1ë…„ê°„ ìµœëŒ€ 720ë§Œì› ì§€ì› \n",
      "ë° í•´ë‹¹ ê¸°ì—…ì— ì·¨ì—… í›„ 18ê°œì›” ì´ìƒ ì¬ì§í•œ ì²­ë…„ ê·¼ë¡œìì—ê²Œ ìµœëŒ€ 480ë§Œì› \n",
      "å¤©Iì›ï¼ˆ18.24ê°œì›”å¤§åœ ê° ìƒ0ë§Œì›ï¼‰\n",
      "ì‚¬ì—… ìš´ì˜ ê¸°ê°„\n",
      "2025ë…„ 1ì›” 1ì¼ ã€œ 2025ë…„ 12ì›” 31ì¼\n",
      "ì‚¬ì—… ì‹ ì²­ ê¸°ê°„\n",
      "2025ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 12ì›” 31ì¼<br/>\n",
      "ì§€ì› ê·œëª¨ï¼ˆëª…ï¼‰\n",
      "ì œí•œ ì—†ìŒ\n",
      "ë¹„ê³ \n",
      "1í˜ì´ì§€\n",
      "------------------------------\n",
      "2. ìœ ì‚¬ë„ ì ìˆ˜: 1.3831\n",
      "   ë‚´ìš©: ì •ì±… ê¸°ë³¸ ì •ë³´\n",
      "\n",
      "ì •ì±…ëª…\n",
      "\n",
      "ì •ì±…ì†Œê°œ\n",
      "\n",
      "ì •ì±… ë¶„ì•¼\n",
      "\n",
      "ì§€ì› ë‚´ìš©\n",
      "\n",
      "ì‚¬ì—… ìš´ì˜ ê¸°ê°„\n",
      "ì‚¬ì—… ì‹ ì²­ ê¸°ê°„\n",
      "ì§€ì› ê·œëª¨(ëª…)\n",
      "\n",
      "ë¹„ê³ \n",
      "\n",
      "ì²­ë…„ì •ì±… ì •ë³´\n",
      "\n",
      "ì²¨ë…„ì¼ìë¦¬ ë„ì•½ì¥ë ¤ê¸ˆ\n",
      "\n",
      "ê¸°ì—…ì˜ ì²­ë…„ê³ ìš© SHS ì§€ì›í•˜ê³ , ì·¨ì—…ì• ë¡œ ì²­ë…„ì˜ ì·¨ì—…ì„ ì´‰ì§„í•¨ìœ¼ë¡œì¨,\n",
      "ADS í™œì„±í™”ë¥¼ ëª©ì ìœ¼ë¡œ í•˜ëŠ” ì •ì±…\n",
      "\n",
      "0 (ìœ í˜• 1) 5ì¸ ì´ìƒ ì¤‘ì†Œê¸°ì—…ì—ì„œ ì·¨ì—…ì• ë¡œì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ìš©í•˜ê³ \n",
      "oR ì´ìƒ ê³ ìš©ìœ ì§€ì‹œ ê¸°ì—…ì§€ì›ê¸ˆ ì§€ê¸‰\n",
      "\n",
      "ã…‡ (ìœ í˜•) ë¹ˆì¼ìë¦¬ ì—…ì¢… ì¤‘ì†Œê¸°ì—…ì—ì„œ ì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ìš© í›„ 6ê°œì›”\n",
      "ì´ìƒ ê³ ìš©ìœ ì§€ì‹œ ê¸°ì—…ì§€ì›ê¸ˆ ì§€ê¸‰í•˜ê³ , í•´ë‹¹ ê¸°ì—…ì—ì„œ 18ê°œì›” ì´ìƒ ì¬ì§í•œ\n",
      "ì²¨ë…„ì—ê²Œ ì²­ë…„ì¥ê¸°ê·¼ì†ì¸ì„¼í‹°ë¸Œ ì§€ê¸‰\n",
      "\n",
      "ì¼ìë¦¬\n",
      "\n",
      "Â© (ìœ í˜• 1) 5ì¸ ì´ìƒ ìš°ì„ ì§€ì›ëŒ€ìƒê¸°ì—…ì—ì„œ ì·¨ì—…ì• ë¡œì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„\n",
      "\n",
      "ìš©ì‹œ 1ë…„ê°„ ìµœëŒ€ 720ë§Œì› ì§€ì›<6[> *ì·¨ì—…ì• ë¡œì •ë…„: ë§Œ 15-34ì„¸ì˜ 4ê°œì›”\n",
      "ì´ìƒ ì‹¤ì—…, ê³ ì¡¸ ì´í•˜ ì •ë…„ ë“± <br> ã…‡(ìœ í˜•2) 5ì¸ ì´ìƒì˜ ì œì¡°ì—… ë“± ë¹ˆì¼ì\n",
      "\n",
      "ë¦¬ ì—…ì¢… ì¤‘ì†Œê¸°ì—…ì—ì„œ BAS ì •ê·œì§ìœ¼ë¡œ ì±„ìš©ì‹œ 1ë…„ê°„ ìµœëŒ€ 720ë§Œì› ì§€ì›\n",
      "\n",
      "ë° í•´ë‹¹ ê¸°ì—…ì— ì·¨ì—… í›„ 18ê°œì›” ì´ìƒ ì¬ì§í•œ ì²­ë…„ ê·¼ë¡œìì—ê²Œ ìµœëŒ€ 480ë§Œì›\n",
      "\n",
      "ì§€ì›(18.24ê°œì›”ì°¨ ê° 240ë§Œì›)\n",
      "\n",
      "2025ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 128 31ì¼\n",
      "2025ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 12ì›” 31ì¼<61/>\n",
      "\n",
      "ì œí•œì—†ìŒ\n",
      "\n",
      "1í˜ì´ì§€\n",
      "------------------------------\n",
      "3. ìœ ì‚¬ë„ ì ìˆ˜: 1.4666\n",
      "   ë‚´ìš©: â–  ì‹ ì²­ ìê²©\n",
      "ì—°ë ¹\n",
      "ë§Œ 15ì„¸ã€œë§Œ 34ì„¸\n",
      "ê±°ì£¼ì§€ ë° ì†Œë“\n",
      "ï¼»ê±°ì£¼ì§€ï¼½\n",
      "ì „êµ­\n",
      "ï¼»ì†Œë“ï¼½\n",
      "ï¼ˆê¸°ì—…ï¼‰ë§¤ì¶œì•¡ ê¸°ì¤€ ï¼ˆì²­ë…„ï¼‰í•´ë‹¹ì—†ìŒ\n",
      "í•™ ë ¥\n",
      "ì œí•œ ì—†ìŒ\n",
      "ì „ê³µ\n",
      "ì œí•œ ì—†ìŒ\n",
      "ì·¨ì—… ìƒíƒœ\n",
      "ì¬ì§ì\n",
      "íŠ¹í™” ë¶„ì•¼\n",
      "ì œí•œ ì—†ìŒ\n",
      "2í˜ì´ì§€\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\n",
    "def search_with_scores(query, k=3):\n",
    "    \"\"\"\n",
    "    ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    # ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\n",
    "    results_with_scores = vectorstore.similarity_search_with_score(query=query, k=k)\n",
    "\n",
    "    return results_with_scores\n",
    "\n",
    "\n",
    "# ì ìˆ˜ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤í–‰\n",
    "query = \"ì²­ë…„ì •ì±…ì´ ë­”ê°€ìš”?\"\n",
    "scored_results = search_with_scores(query)\n",
    "\n",
    "print(f\"ê²€ìƒ‰ ì¿¼ë¦¬: {query}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, (doc, score) in enumerate(scored_results, 1):\n",
    "    print(f\"{i}. ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}\")\n",
    "    print(f\"   ë‚´ìš©: {doc.page_content}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87f4f92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_20516\\3019306973.py:45: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.3)  # ë˜ëŠ” ì‚¬ìš© ì¤‘ì¸ LLM\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_20516\\3019306973.py:38: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  formatted_output = llm(formatted_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. ë¹„ê³ ë€ì€ ì‚­ì œí•´ì£¼ì„¸ìš”\n",
      "\n",
      "# ì²­ë…„ì •ì±… ì •ë³´\n",
      "\n",
      "## ê²°ê³¼ 1 (ìœ ì‚¬ë„: 1.2781)\n",
      "- **ì •ì±…ëª…**: ì²­ë…„ì¼ìë¦¬ ë„ì•½ì¥ë ¤ê¸ˆ\n",
      "- **ì •ì±…ì†Œê°œ**: ê¸°ì—…ì˜ ì²­ë…„ê³ ìš© í™•ëŒ€ë¥¼ ì§€ì›í•˜ê³ , ì·¨ì—…ì• ë¡œ ì²­ë…„ì˜ ì·¨ì—…ì„ ï¿½ï¿½ì§„í•¨ìœ¼ë¡œì¨ ì²­ë…„ê³ ìš© í™œì„±í™”ë¥¼ ëª©ì ìœ¼ë¡œ í•¨.\n",
      "- **ì •ì±… ë¶„ì•¼**: ì¼ìë¦¬\n",
      "- **ì§€ì› ë‚´ìš©**:\n",
      "  - **ìœ í˜• 1**: 5ì¸ ì´ìƒ ìš°ì„ ì§€ì›ëŒ€ìƒê¸°ì—…ì—ì„œ ì·¨ì—…ì• ë¡œì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ìš© ì‹œ 1ë…„ê°„ ìµœëŒ€ 720ë§Œì› ì§€ì›.\n",
      "  - **ìœ í˜• 2**: ë¹ˆì¼ìë¦¬ ì—…ì¢… ì¤‘ì†Œê¸°ì—…ì—ì„œ ì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ìš© í›„ 6ê°œì›” ì´ìƒ ê³ ìš© ìœ ì§€ ì‹œ ê¸°ì—… ì§€ì›ê¸ˆ ì§€ê¸‰ ë° í•´ë‹¹ ê¸°ì—…ì—ì„œ 18ê°œì›” ì´ìƒ ì¬ì§í•œ ì²­ë…„ì—ê²Œ ì²­ë…„ ì¥ê¸°ê·¼ì† ì¸ì„¼í‹°ë¸Œ ì§€ê¸‰.\n",
      "- **ì‚¬ì—… ìš´ì˜ ê¸°ê°„**: 2025ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 12ì›” 31ì¼\n",
      "- **ì‚¬ì—… ì‹ ì²­ ê¸°ê°„**: 202\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI  # ë˜ëŠ” ì‚¬ìš© ì¤‘ì¸ LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def format_search_results_with_llm(scored_results, llm, query=\"\"):\n",
    "    \"\"\"\n",
    "    LLMì„ ì‚¬ìš©í•´ì„œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…\n",
    "    \"\"\"\n",
    "    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    results_text = \"\"\n",
    "    for i, (doc, score) in enumerate(scored_results, 1):\n",
    "        results_text += f\"ê²°ê³¼ {i} (ìœ ì‚¬ë„: {score:.4f}):\\n{doc.page_content}\\n\\n\"\n",
    "\n",
    "    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"results\"],\n",
    "        template=\"\"\"\n",
    "ë‹¤ìŒì€ '{query}' ì§ˆë¬¸ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.\n",
    "ì´ ê²°ê³¼ë“¤ì„ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•´ì„œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ê²€ìƒ‰ ê²°ê³¼:\n",
    "{results}\n",
    "\n",
    "ìš”êµ¬ì‚¬í•­:\n",
    "1. ê° ê²°ê³¼ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•´ì£¼ì„¸ìš”\n",
    "2. ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”\n",
    "3. ë‚´ìš©ì„ ìš”ì•½í•˜ê³  í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ê°•ì¡°í•´ì£¼ì„¸ìš”\n",
    "4. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    # LLMì—ê²Œ í¬ë§·íŒ… ìš”ì²­\n",
    "    formatted_prompt = prompt_template.format(query=query, results=results_text)\n",
    "    formatted_output = llm(formatted_prompt)\n",
    "\n",
    "    return formatted_output\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "query = \"ì²­ë…„ì •ì±…ì´ ë­”ê°€ìš”?\" # ì‚¬ìš©ì ì§ˆë¬¸\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.3)  # ë˜ëŠ” ì‚¬ìš© ì¤‘ì¸ LLM\n",
    "formatted_result = format_search_results_with_llm(scored_results, llm, query)\n",
    "print(formatted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfc6e8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_20516\\3079012876.py:37: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  return LLMChain(llm=llm, prompt=prompt)\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_20516\\3079012876.py:51: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  formatted_output = formatter_chain.run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\n",
      "## ï¿½ï¿½ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½\n",
      "\n",
      "### ï¿½ï¿½ ì§ˆë¬¸: ì²­ë…„ì •ì±…ì´ ï¿½ï¿½ï¿½ê°€ìš”?\n",
      "### ï¿½ï¿½ ì´ 3ê°œ ê²°ê³¼ ë°œê²¬\n",
      "\n",
      "### ï¿½ï¿½ï¿½ ìƒì„¸ ê²°ê³¼:\n",
      "\n",
      "1. **ì œëª©/ìš”ì•½**: ì²­ë…„ì¼ìë¦¬ ë„ì•½ì¥ë ¤ê¸ˆ\n",
      "   - **ï¿½ï¿½ì‹¬ ë‚´ìš©**: ì´ ì •ì±…ì€ ê¸°ì—…ì˜ ì²­ë…„ ê³ ìš©ì„ í™•ëŒ€í•˜ê³  ì·¨ì—… ì• ë¡œ ì²­ë…„ì˜ ì·¨ì—…ì„ ï¿½ï¿½ì§„í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. 5ì¸ ì´ìƒì˜ ì¤‘ì†Œê¸°ì—…ì—ì„œ ì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ìš©í•˜ê³  6ê°œì›” ì´ìƒ ê³ ìš©ì„ ìœ ì§€í•  ê²½ìš° ê¸°ì—…ì— ì§€ì›ê¸ˆì„ ì§€ê¸‰í•©ë‹ˆë‹¤.\n",
      "   - **ìœ ì‚¬ë„ ì ìˆ˜**: 1.2781\n",
      "   - **ê´€ë ¨ì„±**: ë†’ìŒ\n",
      "\n",
      "2. **ì œëª©/ìš”ì•½**: ì²­ë…„ì¼ìë¦¬ ë„ì•½ì¥ë ¤ê¸ˆ (ì •ì±… ê¸°ë³¸ ì •ë³´)\n",
      "   - **ï¿½ï¿½ì‹¬ ë‚´ìš©**: ì´ ì •ì±…ì€ ì²­ë…„ ê³ ìš©ì„ ì§€ì›í•˜ê³  ì·¨ì—… ì• ë¡œ ì²­ë…„ì˜ ì·¨ì—…ì„ ï¿½ï¿½ì§„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. 5ì¸ ì´ìƒ ì¤‘ì†Œê¸°ì—…ì—ì„œ ì²­ë…„ì„ ì •ê·œì§ìœ¼ë¡œ ì±„ìš©í•  ê²½ìš° ì§€ì›ì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\n",
      "   - **ìœ ì‚¬ë„\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def create_result_formatter_chain(llm):\n",
    "    \"\"\"\n",
    "    ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…ì„ ìœ„í•œ ì²´ì¸ ìƒì„±\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"results\", \"result_count\"],\n",
    "        template=\"\"\"\n",
    "ì‚¬ìš©ì ì§ˆë¬¸: \"{query}\"\n",
    "\n",
    "ë‹¤ìŒì€ {result_count}ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤:\n",
    "\n",
    "{results}\n",
    "\n",
    "ìœ„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:\n",
    "\n",
    "## ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½\n",
    "\n",
    "### ğŸ” ì§ˆë¬¸: {query}\n",
    "### ğŸ“Š ì´ {result_count}ê°œ ê²°ê³¼ ë°œê²¬\n",
    "\n",
    "### ğŸ“ ìƒì„¸ ê²°ê³¼:\n",
    "\n",
    "ê° ê²°ê³¼ì— ëŒ€í•´:\n",
    "1. **ì œëª©/ìš”ì•½** (í•œ ì¤„ë¡œ)\n",
    "2. **í•µì‹¬ ë‚´ìš©** (2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½)\n",
    "3. **ìœ ì‚¬ë„ ì ìˆ˜** í‘œì‹œ\n",
    "4. **ê´€ë ¨ì„±** í‰ê°€\n",
    "\n",
    "ë§ˆì§€ë§‰ì— ì „ì²´ì ì¸ **ì¢…í•© ì˜ê²¬**ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    return LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ë°©ë²•\n",
    "def format_with_chain(scored_results, llm, user_query):\n",
    "    formatter_chain = create_result_formatter_chain(llm)\n",
    "\n",
    "    # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "    results_text = \"\"\n",
    "    for i, (doc, score) in enumerate(scored_results, 1):\n",
    "        results_text += f\"**ê²°ê³¼ {i}** (ìœ ì‚¬ë„: {score:.4f})\\n\"\n",
    "        results_text += f\"ë‚´ìš©: {doc.page_content[:200]}...\\n\\n\"\n",
    "\n",
    "    # ì²´ì¸ ì‹¤í–‰\n",
    "    formatted_output = formatter_chain.run(\n",
    "        query=user_query, results=results_text, result_count=len(scored_results)\n",
    "    )\n",
    "\n",
    "    return formatted_output\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "query = \"ì²­ë…„ì •ì±…ì´ ë­”ê°€ìš”?\"  # ì‚¬ìš©ì ì§ˆë¬¸\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.3)  # ë˜ëŠ” ì‚¬ìš© ì¤‘ì¸ LLM\n",
    "formatted_result = format_with_chain(scored_results, llm, query)\n",
    "print(formatted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f344e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– AIê°€ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...\n",
      "\n",
      "==================================================\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "def stream_formatted_results(scored_results, llm, query):\n",
    "    \"\"\"\n",
    "    ì‹¤ì‹œê°„ìœ¼ë¡œ í¬ë§·íŒ…ëœ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°\n",
    "    \"\"\"\n",
    "    # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì„¤ì •\n",
    "    streaming_llm = llm\n",
    "    streaming_llm.callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "    # ê²°ê³¼ ì¤€ë¹„\n",
    "    results_summary = f\"ì´ {len(scored_results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\\n\\n\"\n",
    "\n",
    "    for i, (doc, score) in enumerate(scored_results, 1):\n",
    "        results_summary += f\"ë¬¸ì„œ {i}: ìœ ì‚¬ë„ {score:.4f}\\n\"\n",
    "        results_summary += f\"ë‚´ìš©: {doc.page_content}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:\n",
    "\n",
    "ì§ˆë¬¸: {query}\n",
    "\n",
    "{results_summary}\n",
    "\n",
    "ìš”êµ¬ì‚¬í•­:\n",
    "- ê° ê²°ê³¼ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„\n",
    "- í•µì‹¬ ë‚´ìš© ìš”ì•½\n",
    "- ìœ ìš©í•œ ì •ë³´ ê°•ì¡°\n",
    "- ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±\n",
    "\"\"\"\n",
    "\n",
    "    print(\"ğŸ¤– AIê°€ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...\\n\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥\n",
    "    streaming_llm(prompt)\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "query = \"ì²­ë…„ì •ì±…ì´ ë­”ê°€ìš”?\"  # ì‚¬ìš©ì ì§ˆë¬¸\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.3)  # ë˜ëŠ” ì‚¬ìš© ì¤‘ì¸ LLM\n",
    "formatted_result = stream_formatted_results(scored_results, llm, query)\n",
    "print(formatted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f5d12c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse FormattedSearchResults from completion {\"query\": \"\\uccad\\ub144\\uc815\\ucc45\\uc774 \\ufffd\\ufffd\\ufffd\\uac00\\uc694?\", \"total_results\": 3, \"results\": [{\"rank\": 1, \"similarity_score\": 1.2781, \"title\": \"\\uccad\\ub144\\uc815\\ucc45 \\uc815\\ubcf4\", \"summary\": \"\\uccad\\ub144\\uc77c\\uc790\\ub9ac \\ub3c4\\uc57d\\uc7a5\\ub824\\uae08 \\uc815\\ucc45\\uc740 \\uae30\\uc5c5\\uc758 \\uccad\\ub144\\uace0\\uc6a9 \\ud655\\ub300\\ub97c \\uc9c0\\uc6d0\\ud558\\uace0 \\ucde8\\uc5c5\\uc560\\ub85c \\uccad\\ub144\\uc758 \\ucde8\\uc5c5\\uc744 \\ufffd\\ufffd\\uc9c4\\ud558\\ub294 \\uac83\\uc744 \\ubaa9\\uc801\\uc73c\\ub85c \\ud55c\\ub2e4.\", \"key_points\": [\"\\uc815\\ucc45\\uba85: \\uccad\\ub144\\uc77c\\uc790\\ub9ac \\ub3c4\\uc57d\\uc7a5\\ub824\\uae08\", \"\\uc815\\ucc45 \\ubd84\\uc57c: \\uc77c\\uc790\\ub9ac\", \"\\uc9c0\\uc6d0 \\ub0b4\\uc6a9: 5\\uc778 \\uc774\\uc0c1 \\uc911\\uc18c\\uae30\\uc5c5\\uc5d0\\uc11c \\ucde8\\uc5c5\\uc560\\ub85c\\uccad\\ub144\\uc744 \\uc815\\uaddc\\uc9c1\\uc73c\\ub85c \\ucc44\\uc6a9 \\uc2dc \\ucd5c\\ub300 720\\ub9cc\\uc6d0 \\uc9c0\\uc6d0\", \"\\uc0ac\\uc5c5 \\uc6b4\\uc601 \\uae30\\uac04: 2025\\ub144 1\\uc6d4 1\\uc77c ~ 2025\\ub144 12\\uc6d4 31\\uc77c\", \"\\uc0ac\\uc5c5 \\uc2e0\\uccad \\uae30\\uac04: 2025\\ub144 1\\uc6d4 1\\uc77c ~ 2025\\ub144 12\\uc6d4 31\\uc77c\", \"\\uc9c0\\uc6d0 \\uaddc\\ubaa8: \\uc81c\\ud55c \\uc5c6\\uc74c\"]}, {\"rank\": 2}]}. Got: 5 validation errors for FormattedSearchResults\nresults.1.similarity_score\n  Field required [type=missing, input_value={'rank': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresults.1.title\n  Field required [type=missing, input_value={'rank': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresults.1.summary\n  Field required [type=missing, input_value={'rank': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresults.1.key_points\n  Field required [type=missing, input_value={'rank': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\noverall_summary\n  Field required [type=missing, input_value={'query': 'ì²­ë…„ì •ì±…...ì—†ìŒ']}, {'rank': 2}]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:28\u001b[39m, in \u001b[36mPydanticOutputParser._parse_obj\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m.pydantic_object, pydantic.BaseModel):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpydantic_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m.pydantic_object, pydantic.v1.BaseModel):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\pydantic\\main.py:705\u001b[39m, in \u001b[36mBaseModel.model_validate\u001b[39m\u001b[34m(cls, obj, strict, from_attributes, context, by_alias, by_name)\u001b[39m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    701\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    702\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    703\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 5 validation errors for FormattedSearchResults\nresults.1.similarity_score\n  Field required [type=missing, input_value={'rank': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresults.1.title\n  Field required [type=missing, input_value={'rank': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresults.1.summary\n  Field required [type=missing, input_value={'rank': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresults.1.key_points\n  Field required [type=missing, input_value={'rank': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\noverall_summary\n  Field required [type=missing, input_value={'query': 'ì²­ë…„ì •ì±…...ì—†ìŒ']}, {'rank': 2}]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOutputParserException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# LLM í¬ë§·íŒ… ì‚¬ìš©\u001b[39;00m\n\u001b[32m     62\u001b[39m user_question = \u001b[33m\"\u001b[39m\u001b[33mì²­ë…„ì •ì±…ì´ ë­”ê°€ìš”?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m formatted_output = \u001b[43mstructured_format_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscored_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_question\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(formatted_output)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mstructured_format_results\u001b[39m\u001b[34m(scored_results, llm, query)\u001b[39m\n\u001b[32m     32\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[33më‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:\u001b[39m\n\u001b[32m     34\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mparser.get_format_instructions()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     42\u001b[39m     output = llm(prompt)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     parsed_result = \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\u001b[39;00m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m## ğŸ” ê²€ìƒ‰ ê²°ê³¼: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_result.query\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:77\u001b[39m, in \u001b[36mPydanticOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> TBaseModel:\n\u001b[32m     69\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the output of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m     71\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m \u001b[33;03m        The parsed pydantic object.\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:96\u001b[39m, in \u001b[36mJsonOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m     88\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the output of an LLM call to a JSON object.\u001b[39;00m\n\u001b[32m     89\u001b[39m \n\u001b[32m     90\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m \u001b[33;03m        The parsed JSON object.\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:62\u001b[39m, in \u001b[36mPydanticOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     61\u001b[39m     json_object = \u001b[38;5;28msuper\u001b[39m().parse_result(result)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m partial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:35\u001b[39m, in \u001b[36mPydanticOutputParser._parse_obj\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (pydantic.ValidationError, pydantic.v1.ValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parser_exception(e, obj) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOutputParserException\u001b[39m: Failed to parse FormattedSearchResults from completion {\"query\": \"\\uccad\\ub144\\uc815\\ucc45\\uc774 \\ufffd\\ufffd\\ufffd\\uac00\\uc694?\", \"total_results\": 3, \"results\": [{\"rank\": 1, \"similarity_score\": 1.2781, \"title\": \"\\uccad\\ub144\\uc815\\ucc45 \\uc815\\ubcf4\", \"summary\": \"\\uccad\\ub144\\uc77c\\uc790\\ub9ac \\ub3c4\\uc57d\\uc7a5\\ub824\\uae08 \\uc815\\ucc45\\uc740 \\uae30\\uc5c5\\uc758 \\uccad\\ub144\\uace0\\uc6a9 \\ud655\\ub300\\ub97c \\uc9c0\\uc6d0\\ud558\\uace0 \\ucde8\\uc5c5\\uc560\\ub85c \\uccad\\ub144\\uc758 \\ucde8\\uc5c5\\uc744 \\ufffd\\ufffd\\uc9c4\\ud558\\ub294 \\uac83\\uc744 \\ubaa9\\uc801\\uc73c\\ub85c \\ud55c\\ub2e4.\", \"key_points\": [\"\\uc815\\ucc45\\uba85: \\uccad\\ub144\\uc77c\\uc790\\ub9ac \\ub3c4\\uc57d\\uc7a5\\ub824\\uae08\", \"\\uc815\\ucc45 \\ubd84\\uc57c: \\uc77c\\uc790\\ub9ac\", \"\\uc9c0\\uc6d0 \\ub0b4\\uc6a9: 5\\uc778 \\uc774\\uc0c1 \\uc911\\uc18c\\uae30\\uc5c5\\uc5d0\\uc11c \\ucde8\\uc5c5\\uc560\\ub85c\\uccad\\ub144\\uc744 \\uc815\\uaddc\\uc9c1\\uc73c\\ub85c \\ucc44\\uc6a9 \\uc2dc \\ucd5c\\ub300 720\\ub9cc\\uc6d0 \\uc9c0\\uc6d0\", \"\\uc0ac\\uc5c5 \\uc6b4\\uc601 \\uae30\\uac04: 2025\\ub144 1\\uc6d4 1\\uc77c ~ 2025\\ub144 12\\uc6d4 31\\uc77c\", \"\\uc0ac\\uc5c5 \\uc2e0\\uccad \\uae30\\uac04: 2025\\ub144 1\\uc6d4 1\\uc77c ~ 2025\\ub144 12\\uc6d4 31\\uc77c\", \"\\uc9c0\\uc6d0 \\uaddc\\ubaa8: \\uc81c\\ud55c \\uc5c6\\uc74c\"]}, {\"rank\": 2}]}. Got: 5 validation errors for FormattedSearchResults\nresults.1.similarity_score\n  Field required [type=missing, input_value={'rank': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresults.1.title\n  Field required [type=missing, input_value={'rank': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresults.1.summary\n  Field required [type=missing, input_value={'rank': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresults.1.key_points\n  Field required [type=missing, input_value={'rank': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\noverall_summary\n  Field required [type=missing, input_value={'query': 'ì²­ë…„ì •ì±…...ì—†ìŒ']}, {'rank': 2}]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    rank: int\n",
    "    similarity_score: float\n",
    "    title: str\n",
    "    summary: str\n",
    "    key_points: List[str]\n",
    "\n",
    "\n",
    "class FormattedSearchResults(BaseModel):\n",
    "    query: str\n",
    "    total_results: int\n",
    "    results: List[SearchResult]\n",
    "    overall_summary: str\n",
    "\n",
    "\n",
    "def structured_format_results(scored_results, llm, query):\n",
    "    \"\"\"\n",
    "    êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ê²°ê³¼ í¬ë§·íŒ…\n",
    "    \"\"\"\n",
    "    parser = PydanticOutputParser(pydantic_object=FormattedSearchResults)\n",
    "\n",
    "    # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "    results_text = \"\"\n",
    "    for i, (doc, score) in enumerate(scored_results, 1):\n",
    "        results_text += f\"ê²°ê³¼ {i} (ì ìˆ˜: {score:.4f}): {doc.page_content}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:\n",
    "\n",
    "ì§ˆë¬¸: {query}\n",
    "ê²°ê³¼:\n",
    "{results_text}\n",
    "\n",
    "{parser.get_format_instructions()}\n",
    "\"\"\"\n",
    "\n",
    "    output = llm(prompt)\n",
    "    parsed_result = parser.parse(output)\n",
    "\n",
    "    # ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
    "    print(f\"## ğŸ” ê²€ìƒ‰ ê²°ê³¼: {parsed_result.query}\")\n",
    "    print(f\"ğŸ“Š ì´ {parsed_result.total_results}ê°œ ê²°ê³¼\\n\")\n",
    "\n",
    "    for result in parsed_result.results:\n",
    "        print(f\"### {result.rank}. {result.title}\")\n",
    "        print(f\"**ìœ ì‚¬ë„**: {result.similarity_score:.4f}\")\n",
    "        print(f\"**ìš”ì•½**: {result.summary}\")\n",
    "        print(\"**í•µì‹¬ í¬ì¸íŠ¸**:\")\n",
    "        for point in result.key_points:\n",
    "            print(f\"  â€¢ {point}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(f\"\\n## ğŸ’¡ ì¢…í•© ì˜ê²¬\\n{parsed_result.overall_summary}\")\n",
    "\n",
    "\n",
    "# LLM í¬ë§·íŒ… ì‚¬ìš©\n",
    "user_question = \"ì²­ë…„ì •ì±…ì´ ë­”ê°€ìš”?\"\n",
    "formatted_output = structured_format_results(scored_results, llm, user_question)\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb77366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
