{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a85407cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9f62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# OpenAI API 키 설정 (환경변수 또는 직접 입력)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 1. 임베딩 모델 초기화\n",
    "# OpenAI의 text-embedding-ada-002 모델 사용\n",
    "# 기본 1536차원 : 1536 × 4바이트 = 6.1KB per 문서\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 1024차원으로 축소: 512 × 4바이트 = 2.0KB per 문서 (약 67% 절약)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1024)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408b59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# 1. 문서 로더 설정 (PDF 파일 예시)\n",
    "def load_and_process_document(file_path):\n",
    "    \"\"\"\n",
    "    문서를 로드하고 청크로 분할하는 함수\n",
    "    \"\"\"\n",
    "    # 파일 확장자에 따라 적절한 로더 선택\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 파일 형식입니다.\")\n",
    "\n",
    "    # 문서 로드\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 2. 텍스트 분할기 설정\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # 각 청크의 최대 크기\n",
    "        chunk_overlap=200,  # 청크 간 겹치는 부분\n",
    "        length_function=len,  # 길이 계산 함수\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # 분할 기준\n",
    "    )\n",
    "\n",
    "    # 문서를 청크로 분할\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b1ee718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스캔된 이미지 PDF입니다. OCR이 필요합니다.\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "\n",
    "def check_pdf_type(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "        if text.strip():\n",
    "            print(\"텍스트 PDF입니다.\")\n",
    "        else:\n",
    "            print(\"스캔된 이미지 PDF입니다. OCR이 필요합니다.\")\n",
    "\n",
    "\n",
    "check_pdf_type(\"sample_02.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42552f40",
   "metadata": {},
   "outputs": [
    {
     "ename": "PDFInfoNotInstalledError",
     "evalue": "Unable to get page count. Is poppler installed and in PATH?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\pdf2image\\pdf2image.py:581\u001b[39m, in \u001b[36mpdfinfo_from_path\u001b[39m\u001b[34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[39m\n\u001b[32m    580\u001b[39m     env[\u001b[33m\"\u001b[39m\u001b[33mLD_LIBRARY_PATH\u001b[39m\u001b[33m\"\u001b[39m] = poppler_path + \u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m + env.get(\u001b[33m\"\u001b[39m\u001b[33mLD_LIBRARY_PATH\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m proc = \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python\\Python313\\Lib\\subprocess.py:1039\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1036\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1037\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1039\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1049\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python\\Python313\\Lib\\subprocess.py:1554\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[39m\n\u001b[32m   1553\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1554\u001b[39m     hp, ht, pid, tid = \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[32m   1556\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1559\u001b[39m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1563\u001b[39m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[32m   1564\u001b[39m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1567\u001b[39m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[32m   1568\u001b[39m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] 지정된 파일을 찾을 수 없습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPDFInfoNotInstalledError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m documents\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 사용 예시\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m chunks = \u001b[43mload_and_process_scanned_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msample_02.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m문서 청크 수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mload_and_process_scanned_pdf\u001b[39m\u001b[34m(pdf_path)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_and_process_scanned_pdf\u001b[39m(pdf_path):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# PDF를 이미지로 변환\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     images = \u001b[43mconvert_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# 각 페이지에서 텍스트 추출\u001b[39;00m\n\u001b[32m     12\u001b[39m     full_text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\pdf2image\\pdf2image.py:127\u001b[39m, in \u001b[36mconvert_from_path\u001b[39m\u001b[34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(poppler_path, PurePath):\n\u001b[32m    125\u001b[39m     poppler_path = poppler_path.as_posix()\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m page_count = \u001b[43mpdfinfo_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mownerpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoppler_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpoppler_path\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mPages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# We start by getting the output format, the buffer processing function and if we need pdftocairo\u001b[39;00m\n\u001b[32m    132\u001b[39m parsed_fmt, final_extension, parse_buffer_func, use_pdfcairo_format = _parse_format(\n\u001b[32m    133\u001b[39m     fmt, grayscale\n\u001b[32m    134\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\pdf2image\\pdf2image.py:607\u001b[39m, in \u001b[36mpdfinfo_from_path\u001b[39m\u001b[34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[39m\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PDFInfoNotInstalledError(\n\u001b[32m    608\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to get page count. Is poppler installed and in PATH?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     )\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PDFPageCountError(\n\u001b[32m    612\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to get page count.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr.decode(\u001b[33m'\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    613\u001b[39m     )\n",
      "\u001b[31mPDFInfoNotInstalledError\u001b[39m: Unable to get page count. Is poppler installed and in PATH?"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def load_and_process_scanned_pdf(pdf_path):\n",
    "    # PDF를 이미지로 변환\n",
    "    images = convert_from_path(pdf_path)\n",
    "\n",
    "    # 각 페이지에서 텍스트 추출\n",
    "    full_text = \"\"\n",
    "    for image in images:\n",
    "        text = pytesseract.image_to_string(image, lang=\"kor+eng\")\n",
    "        full_text += text + \"\\n\"\n",
    "\n",
    "    # 텍스트 분할\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "chunks = load_and_process_scanned_pdf(\"sample_02.pdf\")\n",
    "print(f\"문서 청크 수: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3647e143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여전히 오류 발생: Unable to get page count. Is poppler installed and in PATH?\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "try:\n",
    "    images = convert_from_path(\"sample_02.pdf\", first_page=1, last_page=1)\n",
    "    print(\"Poppler 설치 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"여전히 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e02bdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 페이지 수: 5\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "doc = fitz.open(\"sample_02.pdf\")\n",
    "print(f\"PDF 페이지 수: {len(doc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421b8dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 청크 수: 0\n",
      "첫 번째 청크 내용: None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected Embeddings to be non-empty list or numpy array, got [] in upsert.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m문서 청크 수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m첫 번째 청크 내용: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunks[\u001b[32m0\u001b[39m].page_content\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mchunks\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNone\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m vectorstore = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./chroma_db\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mtype\u001b[39m(vectorstore)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:887\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    885\u001b[39m texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    886\u001b[39m metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:843\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m    838\u001b[39m         api=chroma_collection._client,\n\u001b[32m    839\u001b[39m         ids=ids,\n\u001b[32m    840\u001b[39m         metadatas=metadatas,\n\u001b[32m    841\u001b[39m         documents=texts,\n\u001b[32m    842\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    849\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:326\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m         \u001b[38;5;28mself\u001b[39m._collection.upsert(\n\u001b[32m    321\u001b[39m             embeddings=embeddings_without_metadatas,\n\u001b[32m    322\u001b[39m             documents=texts_without_metadatas,\n\u001b[32m    323\u001b[39m             ids=ids_without_metadatas,\n\u001b[32m    324\u001b[39m         )\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:365\u001b[39m, in \u001b[36mCollection.upsert\u001b[39m\u001b[34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupsert\u001b[39m(\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    342\u001b[39m     ids: OneOrMany[ID],\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     uris: Optional[OneOrMany[URI]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    353\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    354\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m \u001b[33;03m        None\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     upsert_request = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_prepare_upsert_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m._client._upsert(\n\u001b[32m    375\u001b[39m         collection_id=\u001b[38;5;28mself\u001b[39m.id,\n\u001b[32m    376\u001b[39m         ids=upsert_request[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m         database=\u001b[38;5;28mself\u001b[39m.database,\n\u001b[32m    383\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:95\u001b[39m, in \u001b[36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, *args: Any, **kwargs: Any) -> T:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     97\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:405\u001b[39m, in \u001b[36mCollectionCommon._validate_and_prepare_upsert_request\u001b[39m\u001b[34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;129m@validation_context\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mupsert\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_and_prepare_upsert_request\u001b[39m(\n\u001b[32m    391\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    403\u001b[39m ) -> UpsertRequest:\n\u001b[32m    404\u001b[39m     \u001b[38;5;66;03m# Unpack\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     upsert_records = \u001b[43mnormalize_insert_record_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m     \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    415\u001b[39m     validate_insert_record_set(record_set=upsert_records)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\chromadb\\api\\types.py:271\u001b[39m, in \u001b[36mnormalize_insert_record_set\u001b[39m\u001b[34m(ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalize_insert_record_set\u001b[39m(\n\u001b[32m    256\u001b[39m     ids: OneOrMany[ID],\n\u001b[32m    257\u001b[39m     embeddings: Optional[\n\u001b[32m   (...)\u001b[39m\u001b[32m    266\u001b[39m     uris: Optional[OneOrMany[URI]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    267\u001b[39m ) -> InsertRecordSet:\n\u001b[32m    268\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[33;03m    Unpacks and normalizes the fields of an InsertRecordSet.\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     base_record_set = \u001b[43mnormalize_base_record_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43muris\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m InsertRecordSet(\n\u001b[32m    276\u001b[39m         ids=cast(IDs, maybe_cast_one_to_many(ids)),\n\u001b[32m    277\u001b[39m         metadatas=maybe_cast_one_to_many(metadatas),\n\u001b[32m   (...)\u001b[39m\u001b[32m    281\u001b[39m         uris=base_record_set[\u001b[33m\"\u001b[39m\u001b[33muris\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    282\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\chromadb\\api\\types.py:248\u001b[39m, in \u001b[36mnormalize_base_record_set\u001b[39m\u001b[34m(embeddings, documents, images, uris)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalize_base_record_set\u001b[39m(\n\u001b[32m    238\u001b[39m     embeddings: Optional[Union[OneOrMany[Embedding], OneOrMany[PyEmbedding]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    239\u001b[39m     documents: Optional[OneOrMany[Document]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    240\u001b[39m     images: Optional[OneOrMany[Image]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    241\u001b[39m     uris: Optional[OneOrMany[URI]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    242\u001b[39m ) -> BaseRecordSet:\n\u001b[32m    243\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[33;03m    Unpacks and normalizes the fields of a BaseRecordSet.\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m BaseRecordSet(\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m         embeddings=\u001b[43mnormalize_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    249\u001b[39m         documents=maybe_cast_one_to_many(documents),\n\u001b[32m    250\u001b[39m         images=maybe_cast_one_to_many(images),\n\u001b[32m    251\u001b[39m         uris=maybe_cast_one_to_many(uris),\n\u001b[32m    252\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workdir\\github-space-cap\\GPT-study-blog\\langchain_project\\venv\\Lib\\site-packages\\chromadb\\api\\types.py:145\u001b[39m, in \u001b[36mnormalize_embeddings\u001b[39m\u001b[34m(target)\u001b[39m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    146\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected Embeddings to be non-empty list or numpy array, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m     )\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    150\u001b[39m     \u001b[38;5;66;03m# One PyEmbedding\u001b[39;00m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target[\u001b[32m0\u001b[39m], (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target[\u001b[32m0\u001b[39m], \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: Expected Embeddings to be non-empty list or numpy array, got [] in upsert."
     ]
    }
   ],
   "source": [
    "# 문서가 제대로 로드되었는지 확인\n",
    "chunks = load_and_process_document(\"sample_02.pdf\")\n",
    "print(f\"문서 청크 수: {len(chunks)}\")\n",
    "print(f\"첫 번째 청크 내용: {chunks[0].page_content if chunks else 'None'}\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks, embedding=embeddings, persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "type(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6df71c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출할 것이 없다.\n",
      "추출할 것이 없다.\n",
      "추출할 것이 없다.\n",
      "추출할 것이 없다.\n",
      "추출할 것이 없다.\n",
      "청크 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# 필요 패키지: pip install PyMuPDF langchain\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def load_and_process_document(file_path: str):\n",
    "    \"\"\"\n",
    "    1) PDF(=텍스트 PDF) → PyMuPDF로 페이지별 텍스트 추출\n",
    "    2) TXT         → open()으로 전체 텍스트 읽기\n",
    "    3) 추출한 문서를 RecursiveCharacterTextSplitter로 청크 분할\n",
    "       (chunk_size=1000, overlap=200)\n",
    "    반환값: List[Document]\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    # ---------- 1. 문서 로드 ----------\n",
    "    if ext == \".pdf\":\n",
    "        raw_docs = []\n",
    "        pdf = fitz.open(file_path)\n",
    "\n",
    "        for page_idx in range(len(pdf)):\n",
    "            page = pdf.load_page(page_idx)\n",
    "            text = page.get_text(\"text\")  # 페이지의 추출 가능한 텍스트\n",
    "            if not text.strip():  # 스캔 이미지 페이지 등은 건너뜀\n",
    "                print(\"추출할 것이 없다.\")\n",
    "                continue\n",
    "\n",
    "            raw_docs.append(\n",
    "                Document(\n",
    "                    page_content=text,\n",
    "                    metadata={\"source\": file_path, \"page\": page_idx + 1},\n",
    "                )\n",
    "            )\n",
    "        pdf.close()\n",
    "\n",
    "    elif ext == \".txt\":\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        raw_docs = [Document(page_content=text, metadata={\"source\": file_path})]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 파일 형식입니다.\")\n",
    "\n",
    "    # ---------- 2. 텍스트 분할 ----------\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_documents(raw_docs)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# 사용 예시 ----------------------------------------------------\n",
    "chunks = load_and_process_document(\"sample_02.pdf\")\n",
    "print(f\"청크 개수: {len(chunks)}\")\n",
    "if chunks:\n",
    "    print(\"첫 번째 청크 내용:\\n\", chunks[0].page_content[:300], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1411490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지 1: 텍스트가 없어 OCR 수행 중...\n",
      "페이지 2: 텍스트가 없어 OCR 수행 중...\n",
      "페이지 3: 텍스트가 없어 OCR 수행 중...\n",
      "페이지 4: 텍스트가 없어 OCR 수행 중...\n",
      "페이지 5: 텍스트가 없어 OCR 수행 중...\n",
      "청크 개수: 5\n",
      "첫 번째 청크 내용:\n",
      " 정책 기본 정보\n",
      "\n",
      "정책명\n",
      "\n",
      "정책소개\n",
      "\n",
      "정책 분야\n",
      "\n",
      "지원 내용\n",
      "\n",
      "사업 운영 기간\n",
      "사업 신청 기간\n",
      "지원 규모(명)\n",
      "\n",
      "비고\n",
      "\n",
      "청년정책 정보\n",
      "\n",
      "첨년일자리 도약장려금\n",
      "\n",
      "기업의 청년고용 SHS 지원하고, 취업애로 청년의 취업을 촉진함으로써,\n",
      "ADS 활성화를 목적으로 하는 정책\n",
      "\n",
      "0 (유형 1) 5인 이상 중소기업에서 취업애로청년을 정규직으로 채용하고\n",
      "oR 이상 고용유지시 기업지원금 지급\n",
      "\n",
      "ㅇ (유형) 빈일자리 업종 중소기업에서 청년을 정규직으로 채용 후 6개월\n",
      "이상 고용유지시 기업지원금 지급하고, 해당 기업에서 18개월 이상 재직한\n",
      "첨년에게  ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Tesseract 실행 파일 경로를 직접 지정\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "def load_and_process_document_with_ocr(file_path: str):\n",
    "    \"\"\"\n",
    "    PDF 문서에서 텍스트를 추출합니다.\n",
    "    텍스트가 없는 페이지는 이미지로 변환 후 OCR 수행.\n",
    "    추출된 텍스트를 LangChain Document로 변환 후 청크 분할하여 반환.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    if ext != \".pdf\":\n",
    "        raise ValueError(\"현재는 PDF 파일만 지원합니다.\")\n",
    "\n",
    "    raw_docs = []\n",
    "    pdf = fitz.open(file_path)\n",
    "\n",
    "    for page_idx in range(len(pdf)):\n",
    "        page = pdf.load_page(page_idx)\n",
    "        text = page.get_text(\"text\")\n",
    "\n",
    "        if not text.strip():\n",
    "            # 텍스트가 없으면 이미지로 변환 후 OCR 수행\n",
    "            print(f\"페이지 {page_idx + 1}: 텍스트가 없어 OCR 수행 중...\")\n",
    "            pix = page.get_pixmap()\n",
    "            img_bytes = pix.tobytes(\"png\")\n",
    "            img = Image.open(io.BytesIO(img_bytes))\n",
    "            ocr_text = pytesseract.image_to_string(img, lang=\"kor+eng\")\n",
    "            text = ocr_text\n",
    "\n",
    "        if text.strip():\n",
    "            raw_docs.append(\n",
    "                Document(\n",
    "                    page_content=text,\n",
    "                    metadata={\"source\": file_path, \"page\": page_idx + 1},\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(f\"페이지 {page_idx + 1}: OCR 후에도 텍스트를 찾을 수 없습니다.\")\n",
    "\n",
    "    pdf.close()\n",
    "\n",
    "    # 텍스트 분할\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_documents(raw_docs)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "chunks = load_and_process_document_with_ocr(\"sample_02.pdf\")\n",
    "print(f\"청크 개수: {len(chunks)}\")\n",
    "if chunks:\n",
    "    print(\"첫 번째 청크 내용:\\n\", chunks[0].page_content[:300], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c0fee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
