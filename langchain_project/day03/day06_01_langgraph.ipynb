{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bbacb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f26dc089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.pregel import NodeBuilder\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, verbose=True)\n",
    "\n",
    "# 인메모리 캐쉬를 위해 MemorySaver를 사용합니다.\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "# 노드 정의\n",
    "def generate_answer(state: ChatState) -> dict:\n",
    "    \"\"\"LLM을 호출하여 답변을 생성하는 노드 함수\"\"\"\n",
    "    print(\"\\n>>>>> LLM 노드 실행! (이 메시지는 캐쉬되지 않았을 때만 보입니다) <<<<<\")\n",
    "    question = state[\"question\"]\n",
    "    response = llm.invoke(question)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# 캐쉬 함수 정의\n",
    "def get_cache_key(state: ChatState, config: dict) -> str:\n",
    "    return state[\"question\"]\n",
    "\n",
    "\n",
    "# 노드 빌더를 이용하여 Cache 기능을 갖는 노드를 정의함\n",
    "# cached_node_builder = NodeBuilder(generate_answer).cache(get_cache_key)\n",
    "\n",
    "# StateGraph 빌더를 생성합니다.\n",
    "builder = StateGraph(ChatState)\n",
    "\n",
    "# NodeBuilder로 생성한 캐쉬 적용 노드를 그래프에 추가합니다.\n",
    "builder.add_node(\"generate_answer_node\", generate_answer)\n",
    "\n",
    "# 엣지 추가: 그래프 흐름 정의\n",
    "builder.add_edge(START, \"generate_answer_node\")  # 시작 -> 답변 생성 노드\n",
    "builder.add_edge(\"generate_answer_node\", END)  # 답변 생성 노드 -> 종료\n",
    "\n",
    "# Checkpointer(캐쉬)를 연결하여 그래프를 컴파일합니다.\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab374fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 첫 번째 질문 ---\n",
      "질문: LangGraph에서 노드 캐싱은 어떻게 하나요?\n",
      "\n",
      ">>>>> LLM 노드 실행! (이 메시지는 캐쉬되지 않았을 때만 보입니다) <<<<<\n",
      "답변: LangGraph에서 노드 캐싱을 구현하는 방법은 여러 가지가 있을 수 있지만, 일반적으로 다음과 같은 접근 방식을 사용할 수 있습니다.\n",
      "\n",
      "1. **메모리 캐싱**: 노드를 메모리에 저장하여 빠르게 접근할 수 있도록 합니다. 예를 들어, Python의 딕셔너리나 리스트를 사용하여 노드를 저장하고, 필요할 때마다 조회합니다.\n",
      "\n",
      "2. **디스크 캐싱**: 노드 데이터를 파일 시스템에 저장하여, 메모리 사용을 줄이고, 애플리케이션이 재시작되더라도 데이터를 유지할 수 있도록 합니다. JSON, CSV, 또는 데이터베이스를 사용할 수 있습니다.\n",
      "\n",
      "3. **캐시 라이브러리 사용**: Redis, Memcached와 같은 외부 캐시 시스템을 사용하여 노드를 저장하고 관리할 수 있습니다. 이러한 시스템은 분산 환경에서도 유용하게 사용할 수 있습니다.\n",
      "\n",
      "4. **TTL (Time to Live)**: 캐시된 노드에 TTL을 설정하여 일정 시간이 지나면 자동으로 삭제되도록 할 수 있습니다. 이를 통해 오래된 데이터를 자동으로 정리할 수 있습니다.\n",
      "\n",
      "5. **변경 감지**: 노드의 변경 사항을 감지하여 캐시를 업데이트하는 방법도 있습니다. 예를 들어, 노드가 수정되면 해당 노드를 캐시에서 제거하거나 업데이트합니다.\n",
      "\n",
      "구체적인 구현 방법은 사용하는 프로그래밍 언어와 프레임워크에 따라 다를 수 있으므로, LangGraph의 문서나 예제를 참조하여 적절한 방법을 선택하는 것이 좋습니다.\n",
      "\n",
      "\n",
      "--- 두 번째 질문 (동일한 내용) ---\n",
      "질문: LangGraph에서 노드 캐싱은 어떻게 하나요?\n",
      "\n",
      ">>>>> LLM 노드 실행! (이 메시지는 캐쉬되지 않았을 때만 보입니다) <<<<<\n",
      "답변: LangGraph에서 노드 캐싱을 구현하는 방법은 여러 가지가 있을 수 있지만, 일반적으로 다음과 같은 접근 방식을 사용할 수 있습니다.\n",
      "\n",
      "1. **메모리 캐싱**: 노드를 메모리에 저장하여 빠르게 접근할 수 있도록 합니다. 예를 들어, Python의 딕셔너리나 리스트를 사용하여 노드를 캐싱할 수 있습니다. 노드를 요청할 때마다 캐시를 확인하고, 캐시에 없으면 데이터베이스나 외부 소스에서 가져와서 캐시에 저장합니다.\n",
      "\n",
      "2. **파일 기반 캐싱**: 노드를 파일 시스템에 저장하여 필요할 때마다 읽어오는 방법입니다. JSON, CSV, 또는 바이너리 형식으로 저장할 수 있습니다. 이 방법은 메모리 사용량을 줄일 수 있지만, I/O 성능에 영향을 줄 수 있습니다.\n",
      "\n",
      "3. **데이터베이스 캐싱**: Redis와 같은 인메모리 데이터베이스를 사용하여 노드를 캐싱할 수 있습니다. 이 방법은 빠른 읽기/쓰기가 가능하며, 분산 시스템에서도 유용합니다.\n",
      "\n",
      "4. **TTL (Time to Live)**: 캐시된 노드에 TTL을 설정하여 일정 시간이 지나면 자동으로 만료되도록 할 수 있습니다. 이를 통해 오래된 데이터를 자동으로 제거하고, 최신 데이터를 유지할 수 있습니다.\n",
      "\n",
      "5. **Lazy Loading**: 노드를 필요할 때만 로드하여 캐싱하는 방법입니다. 처음 요청할 때만 데이터를 가져오고, 이후에는 캐시된 데이터를 사용합니다.\n",
      "\n",
      "구체적인 구현 방법은 LangGraph의 구조와 사용하고 있는 언어, 프레임워크에 따라 달라질 수 있습니다. 따라서, LangGraph의 공식 문서나 커뮤니티 자료를 참고하여 최적의 방법을 선택하는 것이 좋습니다.\n",
      "\n",
      "\n",
      "--- 세 번째 질문 (다른 내용) ---\n",
      "질문: LangChain의 장점은 무엇인가요?\n",
      "\n",
      ">>>>> LLM 노드 실행! (이 메시지는 캐쉬되지 않았을 때만 보입니다) <<<<<\n",
      "답변: LangChain은 자연어 처리(NLP) 및 언어 모델을 활용한 애플리케이션 개발을 위한 프레임워크로, 여러 가지 장점을 가지고 있습니다. 주요 장점은 다음과 같습니다:\n",
      "\n",
      "1. **모듈화**: LangChain은 다양한 구성 요소(예: 프롬프트, 체인, 메모리 등)를 모듈화하여 제공하므로, 개발자가 필요에 따라 쉽게 조합하고 확장할 수 있습니다.\n",
      "\n",
      "2. **유연성**: 다양한 언어 모델과 통합할 수 있어, 특정 요구 사항에 맞는 모델을 선택하여 사용할 수 있습니다. OpenAI, Hugging Face 등 여러 모델과의 호환성을 제공합니다.\n",
      "\n",
      "3. **체인 구성**: LangChain은 여러 작업을 순차적으로 연결할 수 있는 체인 구조를 지원하여 복잡한 작업을 쉽게 처리할 수 있습니다. 예를 들어, 질문 응답, 정보 검색, 데이터 처리 등을 연속적으로 수행할 수 있습니다.\n",
      "\n",
      "4. **상태 관리**: LangChain은 대화의 맥락을 유지하기 위한 메모리 기능을 제공하여, 사용자와의 상호작용에서 일관성을 유지할 수 있습니다.\n",
      "\n",
      "5. **커스터마이징**: 개발자가 자신의 필요에 맞게 프롬프트를 조정하거나, 특정 비즈니스 로직을 추가하여 맞춤형 솔루션을 구축할 수 있습니다.\n",
      "\n",
      "6. **커뮤니티 및 지원**: LangChain은 활발한 커뮤니티와 문서화가 잘 되어 있어, 개발자들이 문제를 해결하고 새로운 아이디어를 얻는 데 도움을 받을 수 있습니다.\n",
      "\n",
      "7. **다양한 응용 분야**: LangChain은 챗봇, 정보 검색 시스템, 자동화된 콘텐츠 생성 등 다양한 분야에 적용할 수 있어, 폭넓은 활용 가능성을 제공합니다.\n",
      "\n",
      "이러한 장점들 덕분에 LangChain은 NLP 기반 애플리케이션을 개발하는 데 매우 유용한 도구로 자리잡고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# ---  그래프 실행 및 캐싱 확인 ---\n",
    "# 대화 스레드를 식별하기 위한 설정\n",
    "config = {\"configurable\": {\"thread_id\": \"chat-thread-1\"}}\n",
    "question = \"LangGraph에서 노드 캐싱은 어떻게 하나요?\"\n",
    "initial_state = {\"question\": question, \"answer\": \"\"}\n",
    "\n",
    "# 첫 번째 호출: LLM 노드가 실행됩니다.\n",
    "print(\"--- 첫 번째 질문 ---\")\n",
    "print(f\"질문: {question}\")\n",
    "final_state = graph.invoke(initial_state, config)\n",
    "print(f\"답변: {final_state['answer']}\")\n",
    "\n",
    "# 두 번째 호출: 동일한 질문이므로 캐쉬된 결과를 반환합니다.\n",
    "print(\"\\n\\n--- 두 번째 질문 (동일한 내용) ---\")\n",
    "print(f\"질문: {question}\")\n",
    "# 'LLM 노드 실행!' 메시지가 출력되지 않아야 합니다.\n",
    "cached_state = graph.invoke(initial_state, config)\n",
    "print(f\"답변: {cached_state['answer']}\")\n",
    "\n",
    "# 세 번째 호출: 다른 질문이므로 LLM 노드가 다시 실행됩니다.\n",
    "print(\"\\n\\n--- 세 번째 질문 (다른 내용) ---\")\n",
    "new_question = \"LangChain의 장점은 무엇인가요?\"\n",
    "new_initial_state = {\"question\": new_question, \"answer\": \"\"}\n",
    "print(f\"질문: {new_question}\")\n",
    "new_final_state = graph.invoke(new_initial_state, config)\n",
    "print(f\"답변: {new_final_state['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba92cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
