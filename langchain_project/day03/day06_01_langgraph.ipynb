{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bbacb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f26dc089",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NodeBuilder.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m state[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 노드 빌더를 이용하여 Cache 기능을 갖는 노드를 정의함\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m cached_node_builder = \u001b[43mNodeBuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_answer\u001b[49m\u001b[43m)\u001b[49m.cache(get_cache_key)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# StateGraph 빌더를 생성합니다.\u001b[39;00m\n\u001b[32m     41\u001b[39m builder = StateGraph(ChatState)\n",
      "\u001b[31mTypeError\u001b[39m: NodeBuilder.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.pregel import NodeBuilder\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, verbose=True)\n",
    "\n",
    "# 인메모리 캐쉬를 위해 MemorySaver를 사용합니다.\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "# 노드 정의\n",
    "def generate_answer(state: ChatState) -> dict:\n",
    "    \"\"\"LLM을 호출하여 답변을 생성하는 노드 함수\"\"\"\n",
    "    print(\"\\n>>>>> LLM 노드 실행! (이 메시지는 캐쉬되지 않았을 때만 보입니다) <<<<<\")\n",
    "    question = state[\"question\"]\n",
    "    response = llm.invoke(question)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# 캐쉬 함수 정의\n",
    "def get_cache_key(state: ChatState, config: dict) -> str:\n",
    "    return state[\"question\"]\n",
    "\n",
    "\n",
    "# 노드 빌더를 이용하여 Cache 기능을 갖는 노드를 정의함\n",
    "cached_node_builder = NodeBuilder(generate_answer).cache(get_cache_key)\n",
    "\n",
    "# StateGraph 빌더를 생성합니다.\n",
    "builder = StateGraph(ChatState)\n",
    "\n",
    "# NodeBuilder로 생성한 캐쉬 적용 노드를 그래프에 추가합니다.\n",
    "builder.add_node(\"generate_answer_node\", cached_node_builder.build())\n",
    "builder.add_edge(START, \"generate_answer_node\")\n",
    "builder.add_edge(\"generate_answer_node\", END)\n",
    "\n",
    "# Checkpointer(캐쉬)를 연결하여 그래프를 컴파일합니다.\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab374fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---  그래프 실행 및 캐싱 확인 ---\n",
    "# 대화 스레드를 식별하기 위한 설정\n",
    "config = {\"configurable\": {\"thread_id\": \"chat-thread-1\"}}\n",
    "question = \"LangGraph에서 노드 캐싱은 어떻게 하나요?\"\n",
    "initial_state = {\"question\": question, \"answer\": \"\"}\n",
    "\n",
    "# 첫 번째 호출: LLM 노드가 실행됩니다.\n",
    "print(\"--- 첫 번째 질문 ---\")\n",
    "print(f\"질문: {question}\")\n",
    "final_state = graph.invoke(initial_state, config)\n",
    "print(f\"답변: {final_state['answer']}\")\n",
    "\n",
    "# 두 번째 호출: 동일한 질문이므로 캐쉬된 결과를 반환합니다.\n",
    "print(\"\\n\\n--- 두 번째 질문 (동일한 내용) ---\")\n",
    "print(f\"질문: {question}\")\n",
    "# 'LLM 노드 실행!' 메시지가 출력되지 않아야 합니다.\n",
    "cached_state = graph.invoke(initial_state, config)\n",
    "print(f\"답변: {cached_state['answer']}\")\n",
    "\n",
    "# 세 번째 호출: 다른 질문이므로 LLM 노드가 다시 실행됩니다.\n",
    "print(\"\\n\\n--- 세 번째 질문 (다른 내용) ---\")\n",
    "new_question = \"LangChain의 장점은 무엇인가요?\"\n",
    "new_initial_state = {\"question\": new_question, \"answer\": \"\"}\n",
    "print(f\"질문: {new_question}\")\n",
    "new_final_state = graph.invoke(new_initial_state, config)\n",
    "print(f\"답변: {new_final_state['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
